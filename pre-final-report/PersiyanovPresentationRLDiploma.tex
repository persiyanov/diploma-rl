\documentclass{beamer}
\usepackage[cp1251]{inputenc}
\usepackage[russian]{babel}
\usepackage{amsmath,mathrsfs,mathtext}
\usepackage{graphicx, epsfig}
\usetheme{Warsaw}%{Singapore}%{Warsaw}%{Darmstadt}
\usecolortheme{sidebartab}
%\definecolor{beamer@blendedblue}{RGB}{15,120,80}
%----------------------------------------------------------------------------------------------------------
\title[\hbox to 56mm{\hfill\insertframenumber\,/\,\inserttotalframenumber}]
{Fine-tuning neural conversation models for auxilary goals by means of deep reinforcement learning}
\author[Д.\,А. Персиянов]{\large \\Дмитрий Андреевич Персиянов}
\institute{\large
Московский физико-технический институт}

%----------------------------------------------------------------------------------------------------------
\begin{document}
%----------------------------------------------------------------------------------------------------------
\begin{frame}
%\thispagestyle{empty}
\titlepage
\end{frame}
%-----------------------------------------------------------------------------------------------------
\begin{frame}{План}
	\begin{itemize}
		\item Conversational модели
		\item RL дообучение
		\item BePolite эксперимент
		\item BeLikeX эксперимент
		\item Заключение и дальнейшие исследования
	\end{itemize}
\end{frame}

%----------------------------------------------------------------------------------------------------------
\begin{frame}{Conversational модели}
В последнее время рекуррентные сети успешно используется для построения языковых и sequence-to-sequence моделей. Обучение происходит на огромных корпусах текстов.

\begin{center}
	\includegraphics[scale=0.3]{imgs/rnn.jpg}
\end{center}


\end{frame}
%------------------------------------------------------------------------------------------------------------
\begin{frame}{Conversational модели}

Имея обучающий пример $(\mathbf{c}, \mathbf{a})$ контекст-ответ, где $\mathbf{c} = \{c_1,c_2,\dots,c_n\},\ \mathbf{a} = \{a_1, a_2,\dots,a_k\}$, учим модель, минимизируя лосс:

$$L(\theta) = -\sum_{t=1}^{k}\log\big(p_{\theta}(a_t \vert a_1,\dots,a_{t-1})\big)$$
или (в RL нотации)
$$L(\theta) = -\mathbb{E}_{\mathbf{a} \sim \mathcal{D}} \big[\log p_{\theta}(\mathbf{a})\big]$$
\end{frame}
%------------------------------------------------------------------------------------------------------------
\begin{frame}{Conversational модели, проблемы}
\begin{itemize}
	\item На один и тот же вопрос два разных ответа (inconsistency)
	\item Выучиваем, минимизируя кроссентропию, а нам иногда хочется другого:
		\begin{itemize}
			\item Консистентность (учитывание контекста предыдущих ответов)
			\item \textbf{Запрет на использование каких-то слов}
			\item \textbf{Ведение беседы в каком-то стиле}
			\item Максимизация скорости завершения диалога
			\item Максимизация удовлетворенности пользователя
			\item Максимизация ...
		\end{itemize}
\end{itemize}


\end{frame}
%------------------------------------------------------------------------------------------------------------
\begin{frame}{RL дообучение}

Диалоговую модель $p_{\theta}(a_t \vert h_t, a_{t-1})$ можно воспринимать как политику $\pi_{\theta}(a_t \vert s_t)$.


Необходимо найти политику $\pi(a \vert s)$, такую что $$\mathbb{E}_{\hat{\mathbf{a}}\sim\pi} \big[R_0+\gamma R_1+\cdots+\gamma^t R_t +\cdots]  \rightarrow max,$$
где $R(\mathbf{a}, \hat{\mathbf{a}})$-- некоторая функция награды, зависящая от правильного ответа $\mathbf{a}$ из обучающей выборки и сгенерированного моделью ответа $\hat{\mathbf{a}}$.

Также возможен более гранулярный вариант $R(a_t, \hat{a}_t)$.

\end{frame}
%------------------------------------------------------------------------------------------------------------
\begin{frame}{BePolite эксперимент}
\begin{itemize}
	\item Данные: opensubtitles.org (en), 18млн пар (контекст, ответ).
	\item Собрали 800 обсценных слов (маты, религиозные/расовые оскорбления). Обозначим это множество за $\mathcal{S}$.
	\item Функция наград: $R(\hat{a}_t) = -\mathbb{I}[\hat{a}_t \in \mathcal{S}]$
	\item Используем предобученную по MLE лоссу модель.
	\item Дообучаем policy-gradient методом по $L(\theta) = -\mathbb{E}_{\mathbf{\hat{a}}\sim p_{\theta}}\big[\sum_{t=1}^kR(\hat{a}_t)\log p_{\theta}(\hat{a}_t \vert \hat{a}_{t-1}, \dots)\big]-\alpha\mathbb{E}_{\mathbf{a} \sim \mathcal{D}} \big[\log p_{\theta}(\mathbf{a})\big]$
	\item $\alpha=5, 20$.
	\item Обучаем 500 батчей по 64 примера (около 30 минут).
\end{itemize}

\end{frame}
%-----------------------------------------------------------------------------------------------------------
\begin{frame}{BePolite эксперимент}

\begin{table}[]
	\centering
	\caption{Метрики бейзлайна}
	\label{bepolite-baseline-table}
	\begin{tabular}{|c|c|}
		\hline
		Средняя награда                   &    Перплексия \\ \hline
		-0.136 &  3.142  \\ \hline
	\end{tabular}
\end{table}

\begin{table}[]
	\centering
	\caption{Метрики после policy-gradient дообучения}
	\label{bepolite-a2c-table}
	\begin{tabular}{|l|c|c|}
		\hline
		$\alpha$ & Средняя награда                   &    Перплексия \\ \hline
		5 & -0.021 &  3.297  \\ \hline
		20 & -0.065 & 3.270 \\ \hline
	\end{tabular}
\end{table}

\end{frame}

%------------------------------------------------------------------------------------------------------------
\begin{frame}{BeLikeX эксперимент}

\begin{itemize}
	\item Данные: twitter (ru), 50млн примеров (контекст, ответ) + каждое сообщение размечено id пользователя.
	\item Отобрали 1000 пользователей по частоте участия в диалогах. (Топ1 -- 9500 ответов на чьи-то твиты).
	\item Обучили dssm-like модель $D(\textbf{uid}, \mathbf{a}) \in [-1, 1]$ в качестве прокси-награды.
	\item Выбрали одного юзера с большим кол-вом сообщений.
\end{itemize}

\end{frame}
%------------------------------------------------------------------------------------------------------------
\begin{frame}{BeLikeX эксперимент}
\begin{table}[]
	\centering
	\caption{Метрики на валидационных выборках}
	\label{belikex-table}
	\begin{tabular}{|l|c|c|c|}
		\hline
		Модель & Перплексия & Перплексия/uid & Средняя награда \\ \hline
		baseline & 4.235 & 5.249 & 0.258 \\ \hline
		llh on user & 5.792 & 6.540 & 0.389 \\ \hline
		dssm weighting & 4.337 & 5.358 & 0.281 \\ \hline
		RL-finetuned & ? & ? & ? \\ \hline
	\end{tabular}
\end{table}

\end{frame}

%----------------------------------------------------------------------------------------------------------
\begin{frame}{Похожие работы}
	\begin{itemize}
		\item Deep Reinforcement Learning for Dialogue Generation (https://arxiv.org/pdf/1612.00563.pdf) -- дообучают RL-ом, но борются с проблемой затухания диалогов и общих ответов.
		\item A Persona-Based Neural Conversation Model (https://nlp.stanford.edu/pubs/jiwei2016Persona.pdf) -- выучивают ембеддинги для пользователей и подают на вход декодеру. \newline
	\end{itemize}
\end{frame}


%----------------------------------------------------------------------------------------------------------
\begin{frame}{Заключение и дальнейшие исследования}

\begin{itemize}
	\item RL помогает быстро и эффективно дообучать модели под разные требования, выразимые в виде функции наград. \newline
	
	\item BePolite: посмотреть как запрет одних слов влияет на частоту использования семантически близких, но которых нет в словаре
	\item BeLikeX: использовать дискриминатор, обученный лишь на одном юзере, как в GAN'ах. Пытаться обмануть его.
\end{itemize}
\end{frame}


%----------------------------------------------------------------------------------------------------------
\end{document} 