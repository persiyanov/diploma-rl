\documentclass[]{article}
\usepackage[utf8]{inputenc}
\usepackage[russian]{babel}
\usepackage{amsmath,amsfonts,mathrsfs,mathtext}
\usepackage{graphicx, epsfig}
\usepackage{geometry}
\geometry{
	a4paper,
	textwidth=145mm,
	top=30mm,
}

%opening
\title{Finetuning Neural Conversational Models for Auxiliary tasks with Deep Reinforcement Learning}
\author{Персиянов Дмитрий, группа 394}


\begin{document}

\date{}
\maketitle

\begin{abstract}
В современном мире большую популярность набрали диалоговые модели на основе рекуррентных нейронных сетей. Обучение моделей происходит на огромных корпусах текстов. К сожалению, для применения их в прикладных задачах (например, чат-поддержка в банке, персональный помощник пользователя) необходимо, чтобы диалоговые модели соответствовали определенным, иногда жестким требованиям. В данной работе предлагается способ дообучения диалоговых моделей под вспомогательные задачи на основе policy-gradient алгоритмов обучения с подкреплением. Предложенная методика не требует большого количества данных и они не требует от них никакого определенного вида, в отличие дообучения по методу максимального правдоподобия.
\end{abstract}

\tableofcontents
\newpage

\section{Введение}

Люди все больше взаимодействуют с компьютером через естественные диалоговые интерфейсы. Классические подходы для построения goal-oriented диалоговых систем (Amazon Alexa, Microsoft Cortana, Google Now, Apple Siri) базируются на понятиях интента и слотов. Они требуют данных из предметной области с соответствующей разметкой интентов и слотов (\cite{DBLP:journals/corr/ZhaoE16}, \cite{rapidly-scaling-dialog-systems-with-interactive-learning}, \cite{DBLP:journals/corr/MrksicSTGSVWY15}, \cite{DBLP:journals/corr/Perez16})

В то же время есть open-domain conversational models, которые в основном работают на sequence-to-sequence моделях (\cite{DBLP:journals/corr/VinyalsL15}, \cite{DBLP:journals/corr/LiGBGD16}, \cite{DBLP:journals/corr/SerbanSBCP15}). Эти модели обучаются на огромных датасетах с диалогами не из предметной области, так как не всегда таковые имеются. Часто возникают задачи дообучать их под дополнительные задачи, которые формализуются с помощью функции награды за сгенерированный ответ.

Интерес к обучению с подкреплением снова вырос за последние два года. Успех его интеграции с глубоким обучением подкрепляется статьями DeepMind  \cite{DBLP:journals/corr/MnihKSGAWR13}, \cite{silver2016mastering}. Обучение с подкреплением позволяет работать с недифференцируемыми функциями наград, что открывает широкие возможности применения таких алгоритмов для дообучения диалоговых моделей (\cite{DBLP:journals/corr/AsadiW16}, \cite{DBLP:journals/corr/BordesW16}, \cite{DBLP:journals/corr/LiMRGGJ16}, \cite{DBLP:journals/corr/LiptonGLLAD16}). 

В данной работе предлагается подход, требующий относительно небольшое количество времени и данных для дообучения моделей и основанный на обучении с подкреплением. Рассматриваются задачи поощрения/запрета списка слов для модели, а также задача генерации ответов в стиле какой-либо персоны.

\section{Нейросетевые диалоговые модели}

Будем рассматривать sequence-to-sequence диалоговые модели на основе рекуррентных нейронных сетей (\cite{DBLP:journals/corr/LiGBGD16}, \cite{DBLP:journals/corr/VinyalsL15}). Такие модели состоят из двух рекуррентных сетей -- энкодер и декодер.

Энкодеру подается на вход предложение $\mathbf{x} = \lbrace x_1, x_2, \dots, x_n\rbrace$, где $x_i$ -- векторное представление $i$-го слова предложения. Энкодер поддерживает внутреннее состояние $\mathbf{h_t} = f(\mathbf{h_{t-1}}, x_t)$, которое изначально инициализируется нулями или случайными числами. После обработки предложения скрытое состояние энкодера $\mathbf{h_n}$ трактуется как латентное представление входного предложения и используется для инициализации скрытого состояния декодера.

Декодер инициализируется последним скрытым состоянием $\mathbf{h_n}$ энкодера, принимает на вход служебный токен $\textbf{BOS}$ и генерирует последовательность слов $\mathbf{\hat{y}} = \lbrace \hat{y}_1, \hat{y}_2, \dots, \hat{y}_m\rbrace$, минимизируя кроссентропию между распределением $p(\mathbf{\hat{y}} | \mathbf{x})$ и истинным дискретным распределением ответа $p(\mathbf{y} | \mathbf{x})$. Слово $\hat{y}_i$ генерируется на основе скрытого состояния декодера $\mathbf{h^{dec}_i}$ и $i-1$-го слова из истинного ответа:

\begin{equation}
p(\hat{y}_i | y_{i-1}, \dots, y_1, \mathbf{x}) = g(\mathbf{h^{dec}_i}, y_{i-1})
\end{equation}

Функцией потерь в задаче обучения диалоговой модели является кроссэнтропия:

\begin{equation}
\label{crossentropy}
L(\theta) =  -\frac{1}{T}\sum_{t=1}^{T}\sum_{j=1}^{|V|} y_{tj} \cdot \log(\hat{y}_{tj}),
\end{equation}
где $|V|$ -- размер словаря, $T$ -- длина последовательности, $y_{t}$ -- one-hot представление правильного $t$-го слова в ответе, а $\hat{y}_{t}$ -- вероятностное распределение, полученное от модели.

\section{Обучение с подкреплением и policy-gradient методы}

В этом разделе ставится задача обучения с подкреплением, описываются policy-gradient методы для ее решения. Также описывается постановка данной задачи в рамках диалоговых моделей.

Назовем \textbf{средой} марковский процесс принятия решений $\mathcal{M} = (\mathcal{S}, \mathcal{A}, P, r, \gamma)$, где $\mathcal{S}$ -- множество (возможно бесконечное) состояний, $\mathcal{A}$ -- множество (возможно бесконечное) допустимых действий агента, $P = P(s' | s,a)$ -- динамика среды, $r = r(s, a)$ -- средняя награда при совершении агентом действия $a$ из состояния $s$, $\gamma$ -- фактор дисконтирования.

Назовем \textbf{агентом} (политикой) распределение $\pi(a|s) : \mathcal{A} \times \mathcal{S} \rightarrow [0, 1]$. Агент взаимодействует со средой во времени. В каждый момент времени $t$ агент находится в состоянии $s_t$, совершает действие $a_t$, получает от среды награду $r_t$ и переходит в следующее состояние $s_{t+1}$. Задача агента -- максимизировать дисконтированную суммарную награду $G_0 = \sum_{i=0}^{T}\gamma^i r_i$. Введем обобщение суммарной награды $G_t = \sum_{i=t}^{T}\gamma^i r_i$.

Будем рассматривать параметризованные стохастические политики $\pi_{\theta}(a|s)$ и пытаться максимизировать суммарную награду. Функционал полезности политики $\eta(\pi) := \mathbb{E}[G_0]$. В ранних работах (\cite{sutton1998reinforcement}, \cite{sutton1999policy}) был установлен основной метод для такой оптимизации, основанный на policy-gradient теореме. Следуя ему, политику можно обновлять стохастическим градиентным спуском:

\begin{equation}
\label{reinforce_pg_eq}
\Delta \theta = \sum_{t=0}^{T-1}G_t \nabla_\theta\log\pi_{\theta}(a_t|s_t)
\end{equation}

Обновления весов в точности по (\ref{reinforce_pg_eq}) приводят к методу REINFORCE. В таком обновлении есть недостаток, связанный с большой дисперсией $G_t$, что свойственно оценкам Монте-Карло. В \cite{sutton1999policy} показано, что выражение

\begin{equation}
\label{a2c_pg_eq}
\Delta \theta = \sum_{t=0}^{T-1}(G_t - V(s)) \nabla_\theta\log\pi_{\theta}(a_t|s_t)
\end{equation}

также является градиентом $\eta(\pi) := \mathbb{E}[G_0]$. Добавка $V(s)$ является оценкой value-function, также называется бейзлайном. Обновление весов по (\ref{a2c_pg_eq}) приводит к серии методов Actor-Critic (или A2C).

В нейросетевой диалоговой модели политикой естественно принять распределение на словах, полученное от декодера. Параметры политики это параметры всей диалоговой модели. Также можно параметризовать политику лишь параметрами декодера, а энкодер оставлять неизменным.

Действиями в данном контексте будут генерируемые слова, а состоянием агента -- скрытое состояние декодера.

\section{Постановка задачи}

Пусть дана диалоговая модель $G_{\theta}: \mathbf{x} \rightarrow \hat{\mathbf{y}}$, отвечающая на сообщение $\mathbf{x}$ сообщением $\mathbf{y}$. Пусть задана функция награды за ответ $R(\mathbf{y}, \hat{\mathbf{y}})$. Нашей задачей будет являться оптимизация параметров $\theta$ с целью максимизации средней награды по выборке $(\mathbf{x}_i, \mathbf{y}_i)_{i=1}^N$:

\begin{equation}
\label{problem_optimize_reward}
\frac{1}{N}\sum_{i=1}^N R(\mathbf{y}_i, G_{\theta}(\mathbf{x}_i)) \xrightarrow[\theta]{} \max
\end{equation}

Заметим, что постановка задачи достаточно общая, что выражается в зависимости функции награды как от правильного ответа, так и от сгенерированного моделью. В наших экспериментах функция наград, как правило, будет содержать два слагаемых, первое из которых соответствует оптимизации модели под конкретную задачу, а второе будет соответствовать сохранению кроссэнтропии на выборке.
\section{Эксперименты}

В качестве диалоговой модели использовалась 1-слойная LSTM сеть с размером скрытого слоя 1024. Последнее состояние энкодера использовалось не только для инициализации декодера, но и подавалось в него на каждом моменте времени. В качестве входа в энкодер подавались 3 последние предложения из контекста. Модель обучалась на английских субтитрах с opensubtitles.org. Размер датасета -- 18 миллионов пар контекст-ответ.

Общая функция потерь состоит из двух слагаемых -- RL функции потерь из \ref{a2c_pg_eq} и LLH функции потерь из стандартной диалоговой модели с каким-то весом $\alpha$ (в экспериментах $\alpha=5$). Это необходимо для того, чтобы распределение декодера не вырождалось и присутствовала языковая структура в ответах.

\subsection{BePolite}

В качестве первого эксперимента была поставлена следующая задача: по данному списку "запрещенных" слов дообучить модель с целью убрать "запрещенные" слова из ответов модели, при этов сохранив языковую структуру в ответе, то есть сохранив кроссэнтропию около значения, которое было достигнуто обучением диалоговой модели. Был собран список из ~250 "запретных" слов. За генерацию любого слова из этого списка агенту давалась награда $r_t = -1$. 

\begin{table}[h]
	\centering
	\begin{tabular}{cccll}
		\multicolumn{1}{c|}{}  & \multicolumn{1}{c|}{Базовый seq2seq} & \multicolumn{1}{c|}{A2C} &  &  \\ \cline{1-3}
		\multicolumn{1}{c|}{All} & \multicolumn{1}{c|}{-0.256} & \multicolumn{1}{c|}{-0.024} &  &  \\ \cline{1-3}
		\multicolumn{1}{c|}{Target list conditioned} & \multicolumn{1}{c|}{-0.293} & \multicolumn{1}{c|}{-0.028} &  &  \\ \cline{1-3}
	\end{tabular}
	\caption{Средние награды базовой модели и дообученной с помощью A2C.}
	\label{table_rewards_obscene}
\end{table}

В таблице \ref{table_rewards_obscene} приведены средние награды за ответ для базовой модели и ее дообученной версии. Строка "All" соответствует средним наградам по любым входным предложениям, строка "Target list conditioned" соответствует средним наградам за ответ по входным предложениям, которые содержат в себе хотя бы одно слово из списка. Базовая модель генерирует хотя бы одно "запрещенное" слово в 25 случаях из 100 (29 из 100 для "Target list conditioned"), а дообученная в 2 из 100 (3 из 100). 

Время дообучения модели составило 0.5 часа на одной видеокарте GTX 1080. Было обработано 800 случайных минибатчей по 64 примера (50 тысяч примеров из 18 миллионов).


<TODO>: Вставить показатели перплексии до и после дообучения, показав что модель не сильно ухудшилась.

\subsection{BeLikeX}

Следующий эксперимент будет заключаться в том, чтобы заставить модель говорить как какой-то человек. Для этого берется дискриминатор $D(\mathbf{x})$, который выдает число -- меру похожести реплики $\mathbf{x}$ на реплики персоны. Обучать его можно, имея примеры фраз нужной персоны и примеры фраз любых других персон. В дальнейшем этот дискриминатор используется как функция награды при обучении модели.

блабла


\newpage
\bibliography{library}
%\bibliographystyle{abbrv}
\bibliographystyle{unsrt}


\end{document}
