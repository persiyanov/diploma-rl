{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import phrase2matrix, tokens, token2idx, UNK_ix, EOS_ix\n",
    "from mymodule.data_stuff import *\n",
    "from collections import deque\n",
    "import pickle\n",
    "import tqdm\n",
    "\n",
    "# %env THEANO_FLAGS=device=gpu5,floatX=float32,exception_verbosity=high,lib.cnmem=0.95,mode=FAST_RUN\n",
    "from warnings import warn\n",
    "import numpy as np\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "from collections import OrderedDict\n",
    "import lasagne\n",
    "\n",
    "from lasagne.layers import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_all_contexts(context_size=2, verbose=100000):\n",
    "    with open('./open_subtitles_en_raw') as fin:\n",
    "        lines = [l.strip() for l in fin]\n",
    "\n",
    "    contexts = []\n",
    "    curr_context = deque(lines[:context_size], context_size)\n",
    "    curr_answer = lines[context_size]\n",
    "    \n",
    "    t = 0\n",
    "    for line in lines[context_size+1:]:\n",
    "        contexts.append({'context':list(curr_context), 'answer': curr_answer})\n",
    "\n",
    "        if t % verbose == 0:\n",
    "            print(t)\n",
    "        curr_context.append(curr_answer)\n",
    "        curr_answer = line.strip()\n",
    "\n",
    "        t += 1\n",
    "    return contexts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('contexts.pkl', 'rb') as fin:\n",
    "    contexts = pickle.load(fin)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load obscene words\n",
    "\n",
    "**Hack for symbolic computation of rewards**:\n",
    "1. Let suppose that we have list of tokens = [25,90,102]\n",
    "2. And we also have generated batch with shape [batch_size, n_steps] with word indices.\n",
    "3. For each word we want to check whether it is equals to any of tokens in our list.\n",
    "\n",
    "We can do this using theano (same as numpy) broadcasting:\n",
    "\n",
    "``T.eq(tokens[None,None,:], batch[:,:,None]).any(-1)``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Some examples ['ejaculate', 'l3i+ch', 'unhappy', 'schlong', 'messy', 's_h_i_t', 'bitchin', 'porno', 'kums', 'assnigger']\n",
      "Number of target words 238\n"
     ]
    }
   ],
   "source": [
    "with open('./obscene_words.txt') as fin:\n",
    "    target_words = set()\n",
    "    for line in fin:\n",
    "        words_in_line = line.strip().split()\n",
    "        if len(words_in_line) == 1:\n",
    "            target_words.update(words_in_line)\n",
    "\n",
    "# target_words = list('.!,?')\n",
    "\n",
    "target_words.add('suck')\n",
    "target_words.add('goddamn')\n",
    "target_words.add('motherfucker')\n",
    "target_words.add('nigger')\n",
    "target_words.add('nigga')\n",
    "target_words.add('ass')\n",
    "target_words.add('crap')\n",
    "target_words.add('fuck')\n",
    "\n",
    "target_words.remove('butt')\n",
    "\n",
    "\n",
    "print(\"Some examples %s\" % list(target_words)[:10])\n",
    "\n",
    "target_idxs = set(filter(lambda x: x != UNK_ix, [token2idx[w] for w in target_words]))\n",
    "\n",
    "target_idxs_shared = theano.shared(np.array(list(target_idxs)))\n",
    "print(\"Number of target words %d\" % len(target_idxs))\n",
    "\n",
    "target_idxs_mask = np.ones((N_TOKENS,))\n",
    "target_idxs_mask[np.array(list(target_idxs))] = 0\n",
    "target_idxs_mask = theano.shared(target_idxs_mask)\n",
    "\n",
    "def _calc_rewards(symbolic_batch):\n",
    "    assert symbolic_batch.ndim == 2\n",
    "    rewards = T.eq(target_idxs_shared[None, None, :], symbolic_batch[:, :, None]).any(-1)\n",
    "    rewards = T.cast(rewards, 'int32')\n",
    "    assert rewards.ndim == 2\n",
    "\n",
    "    # Find EOS_ix in batch\n",
    "    done_mask = T.eq(symbolic_batch, EOS_ix)\n",
    "    # Set done==True for all words after EOS_ix\n",
    "    done_mask = T.concatenate([T.zeros_like(done_mask[:,:1]), done_mask[:,:-1]], axis=1)\n",
    "    \n",
    "    is_alive = T.eq(T.cumsum(done_mask, axis=1), 0).astype('uint8')\n",
    "    return rewards, is_alive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'5h1t',\n",
       " '5hit',\n",
       " 'Eskimo',\n",
       " 'Indian',\n",
       " 'Jerry',\n",
       " 'Kraut',\n",
       " 'LEN',\n",
       " 'Paddy',\n",
       " 'Paki',\n",
       " 'Pygmy',\n",
       " 'Taffy',\n",
       " 'WASP',\n",
       " 'Yank',\n",
       " 'Yankee',\n",
       " 'a2m',\n",
       " 'a55',\n",
       " 'a_s_s',\n",
       " 'adult',\n",
       " 'amateur',\n",
       " 'anal',\n",
       " 'anilingus',\n",
       " 'anus',\n",
       " 'ar5e',\n",
       " 'arrse',\n",
       " 'arse',\n",
       " 'arsehole',\n",
       " 'ass',\n",
       " 'ass-fucker',\n",
       " 'ass-hat',\n",
       " 'ass-hole',\n",
       " 'ass-jabber',\n",
       " 'ass-pirate',\n",
       " 'assbag',\n",
       " 'assbandit',\n",
       " 'assbanger',\n",
       " 'assbite',\n",
       " 'assclown',\n",
       " 'asscock',\n",
       " 'asscracker',\n",
       " 'asses',\n",
       " 'assface',\n",
       " 'assfuck',\n",
       " 'assfucker',\n",
       " 'assfukka',\n",
       " 'assgoblin',\n",
       " 'asshat',\n",
       " 'asshead',\n",
       " 'asshole',\n",
       " 'assholes',\n",
       " 'asshopper',\n",
       " 'assjacker',\n",
       " 'asslick',\n",
       " 'asslicker',\n",
       " 'assmonkey',\n",
       " 'assmucus',\n",
       " 'assmunch',\n",
       " 'assmuncher',\n",
       " 'assnigger',\n",
       " 'asspirate',\n",
       " 'assshit',\n",
       " 'assshole',\n",
       " 'asssucker',\n",
       " 'asswad',\n",
       " 'asswhole',\n",
       " 'asswipe',\n",
       " 'autoerotic',\n",
       " 'axwound',\n",
       " 'b!tch',\n",
       " 'b00bs',\n",
       " 'b17ch',\n",
       " 'b1tch',\n",
       " 'ballbag',\n",
       " 'ballsack',\n",
       " 'bampot',\n",
       " 'bangbros',\n",
       " 'bareback',\n",
       " 'bastard',\n",
       " 'beaner',\n",
       " 'beastial',\n",
       " 'beastiality',\n",
       " 'bellend',\n",
       " 'bestial',\n",
       " 'bestiality',\n",
       " 'bi+ch',\n",
       " 'biatch',\n",
       " 'bimbos',\n",
       " 'birdlock',\n",
       " 'bitch',\n",
       " 'bitchass',\n",
       " 'bitcher',\n",
       " 'bitchers',\n",
       " 'bitches',\n",
       " 'bitchin',\n",
       " 'bitching',\n",
       " 'bitchtits',\n",
       " 'bitchy',\n",
       " 'black',\n",
       " 'bloody',\n",
       " 'blowjob',\n",
       " 'blowjobs',\n",
       " 'blumpkin',\n",
       " 'boiolas',\n",
       " 'bollock',\n",
       " 'bollocks',\n",
       " 'bollok',\n",
       " 'bollox',\n",
       " 'boner',\n",
       " 'boob',\n",
       " 'boobs',\n",
       " 'booobs',\n",
       " 'boooobs',\n",
       " 'booooobs',\n",
       " 'booooooobs',\n",
       " 'breasts',\n",
       " 'brotherfucker',\n",
       " 'buceta',\n",
       " 'bugger',\n",
       " 'bullshit',\n",
       " 'bum',\n",
       " 'bumblefuck',\n",
       " 'busty',\n",
       " 'butt',\n",
       " 'butt-pirate',\n",
       " 'buttfucka',\n",
       " 'buttfucker',\n",
       " 'butthole',\n",
       " 'buttmuch',\n",
       " 'buttplug',\n",
       " 'c0ck',\n",
       " 'c0cksucker',\n",
       " 'carpetmuncher',\n",
       " 'cawk',\n",
       " 'chink',\n",
       " 'choade',\n",
       " 'cipa',\n",
       " 'cl1t',\n",
       " 'clit',\n",
       " 'clitoris',\n",
       " 'clits',\n",
       " 'clusterfuck',\n",
       " 'cnut',\n",
       " 'cock',\n",
       " 'cock-sucker',\n",
       " 'cockface',\n",
       " 'cockhead',\n",
       " 'cockmunch',\n",
       " 'cockmuncher',\n",
       " 'cocks',\n",
       " 'cocksuck',\n",
       " 'cocksucked',\n",
       " 'cocksucker',\n",
       " 'cocksucking',\n",
       " 'cocksucks',\n",
       " 'cocksuka',\n",
       " 'cocksukka',\n",
       " 'cok',\n",
       " 'cokmuncher',\n",
       " 'coksucka',\n",
       " 'colored',\n",
       " 'coloured',\n",
       " 'coolie',\n",
       " 'coon',\n",
       " 'cornhole',\n",
       " 'cox',\n",
       " 'crap',\n",
       " 'cum',\n",
       " 'cumdump',\n",
       " 'cummer',\n",
       " 'cumming',\n",
       " 'cums',\n",
       " 'cumshot',\n",
       " 'cunilingus',\n",
       " 'cunillingus',\n",
       " 'cunnilingus',\n",
       " 'cunt',\n",
       " 'cunt-struck',\n",
       " 'cuntbag',\n",
       " 'cuntlick',\n",
       " 'cuntlicker',\n",
       " 'cuntlicking',\n",
       " 'cunts',\n",
       " 'cuntsicle',\n",
       " 'cyalis',\n",
       " 'cyberfuc',\n",
       " 'cyberfuck',\n",
       " 'cyberfucked',\n",
       " 'cyberfucker',\n",
       " 'cyberfuckers',\n",
       " 'cyberfucking',\n",
       " 'd1ck',\n",
       " 'dago',\n",
       " 'damn',\n",
       " 'dick',\n",
       " 'dickhead',\n",
       " 'dildo',\n",
       " 'dildos',\n",
       " 'dink',\n",
       " 'dinks',\n",
       " 'dirsa',\n",
       " 'dlck',\n",
       " 'dog-fucker',\n",
       " 'doggiestyle',\n",
       " 'doggin',\n",
       " 'dogging',\n",
       " 'donkeyribber',\n",
       " 'doosh',\n",
       " 'duche',\n",
       " 'dyke',\n",
       " 'ejaculate',\n",
       " 'ejaculated',\n",
       " 'ejaculates',\n",
       " 'ejaculating',\n",
       " 'ejaculatings',\n",
       " 'ejaculation',\n",
       " 'ejakulate',\n",
       " 'erotic',\n",
       " 'f4nny',\n",
       " 'f_u_c_k',\n",
       " 'facial',\n",
       " 'fag',\n",
       " 'fagging',\n",
       " 'faggitt',\n",
       " 'faggot',\n",
       " 'faggs',\n",
       " 'fagot',\n",
       " 'fagots',\n",
       " 'fags',\n",
       " 'fanny',\n",
       " 'fannyflaps',\n",
       " 'fannyfucker',\n",
       " 'fanyy',\n",
       " 'fatass',\n",
       " 'fcuk',\n",
       " 'fcuker',\n",
       " 'fcuking',\n",
       " 'feck',\n",
       " 'fecker',\n",
       " 'felching',\n",
       " 'fellate',\n",
       " 'fellatio',\n",
       " 'feringhee',\n",
       " 'fingerfuck',\n",
       " 'fingerfucked',\n",
       " 'fingerfucker',\n",
       " 'fingerfuckers',\n",
       " 'fingerfucking',\n",
       " 'fingerfucks',\n",
       " 'fistfuck',\n",
       " 'fistfucked',\n",
       " 'fistfucker',\n",
       " 'fistfuckers',\n",
       " 'fistfucking',\n",
       " 'fistfuckings',\n",
       " 'fistfucks',\n",
       " 'flange',\n",
       " 'fook',\n",
       " 'fooker',\n",
       " 'frog',\n",
       " 'fuck',\n",
       " 'fuck-ass',\n",
       " 'fuck-bitch',\n",
       " 'fucka',\n",
       " 'fucked',\n",
       " 'fucker',\n",
       " 'fuckers',\n",
       " 'fuckhead',\n",
       " 'fuckheads',\n",
       " 'fuckin',\n",
       " 'fucking',\n",
       " 'fuckings',\n",
       " 'fuckingshitmotherfucker',\n",
       " 'fuckme',\n",
       " 'fuckmeat',\n",
       " 'fucks',\n",
       " 'fucktoy',\n",
       " 'fuckwhit',\n",
       " 'fuckwit',\n",
       " 'fudgepacker',\n",
       " 'fuk',\n",
       " 'fuker',\n",
       " 'fukker',\n",
       " 'fukkin',\n",
       " 'fuks',\n",
       " 'fukwhit',\n",
       " 'fukwit',\n",
       " 'fux',\n",
       " 'fux0r',\n",
       " 'gang-bang',\n",
       " 'gangbang',\n",
       " 'gangbanged',\n",
       " 'gangbangs',\n",
       " 'gaylord',\n",
       " 'gaysex',\n",
       " 'goatse',\n",
       " 'god',\n",
       " 'god-dam',\n",
       " 'god-damned',\n",
       " 'goddamn',\n",
       " 'goddamned',\n",
       " 'gook',\n",
       " 'goy',\n",
       " 'gringo',\n",
       " 'gypsy',\n",
       " 'half-breed',\n",
       " 'half-caste',\n",
       " 'hardcoresex',\n",
       " 'hell',\n",
       " 'heshe',\n",
       " 'hoar',\n",
       " 'hoare',\n",
       " 'hoer',\n",
       " 'homo',\n",
       " 'homoerotic',\n",
       " 'honky',\n",
       " 'hore',\n",
       " 'horniest',\n",
       " 'horny',\n",
       " 'hotsex',\n",
       " 'idiot',\n",
       " 'jack-off',\n",
       " 'jackoff',\n",
       " 'jap',\n",
       " 'jerk',\n",
       " 'jerk-off',\n",
       " 'jism',\n",
       " 'jiz',\n",
       " 'jizm',\n",
       " 'jizz',\n",
       " 'jock',\n",
       " 'kaffir',\n",
       " 'kafir',\n",
       " 'kawk',\n",
       " 'knob',\n",
       " 'knobead',\n",
       " 'knobed',\n",
       " 'knobend',\n",
       " 'knobhead',\n",
       " 'knobjocky',\n",
       " 'knobjokey',\n",
       " 'kock',\n",
       " 'kondum',\n",
       " 'kondums',\n",
       " 'kum',\n",
       " 'kummer',\n",
       " 'kumming',\n",
       " 'kums',\n",
       " 'kunilingus',\n",
       " 'kwif',\n",
       " 'l3i+ch',\n",
       " 'l3itch',\n",
       " 'labia',\n",
       " 'lmao',\n",
       " 'lmfao',\n",
       " 'lust',\n",
       " 'lusting',\n",
       " 'm0f0',\n",
       " 'm0fo',\n",
       " 'm45terbate',\n",
       " 'ma5terb8',\n",
       " 'ma5terbate',\n",
       " 'mafugly',\n",
       " 'makwerekwere',\n",
       " 'malicious',\n",
       " 'masochist',\n",
       " 'master-bate',\n",
       " 'masterb8',\n",
       " 'masterbat*',\n",
       " 'masterbat3',\n",
       " 'masterbate',\n",
       " 'masterbation',\n",
       " 'masterbations',\n",
       " 'masturbate',\n",
       " 'mean',\n",
       " 'menacing',\n",
       " 'messy',\n",
       " 'mick',\n",
       " 'misshapen',\n",
       " 'missing',\n",
       " 'misunderstood',\n",
       " 'mo-fo',\n",
       " 'moan',\n",
       " 'mof0',\n",
       " 'mofo',\n",
       " 'moldy',\n",
       " 'monstrous',\n",
       " 'mothafuck',\n",
       " 'mothafucka',\n",
       " 'mothafuckas',\n",
       " 'mothafuckaz',\n",
       " 'mothafucked',\n",
       " 'mothafucker',\n",
       " 'mothafuckers',\n",
       " 'mothafuckin',\n",
       " 'mothafucking',\n",
       " 'mothafuckings',\n",
       " 'mothafucks',\n",
       " 'motherfuck',\n",
       " 'motherfucked',\n",
       " 'motherfucker',\n",
       " 'motherfuckers',\n",
       " 'motherfuckin',\n",
       " 'motherfucking',\n",
       " 'motherfuckings',\n",
       " 'motherfuckka',\n",
       " 'motherfucks',\n",
       " 'muff',\n",
       " 'mulatto',\n",
       " 'mutha',\n",
       " 'muthafecker',\n",
       " 'muthafuckker',\n",
       " 'muther',\n",
       " 'mutherfucker',\n",
       " 'n1gga',\n",
       " 'n1gger',\n",
       " 'naive',\n",
       " 'nasty',\n",
       " 'native',\n",
       " 'naughty',\n",
       " 'nazi',\n",
       " 'negate',\n",
       " 'negative',\n",
       " 'negress',\n",
       " 'negro',\n",
       " 'never',\n",
       " 'nigg3r',\n",
       " 'nigg4h',\n",
       " 'nigga',\n",
       " 'niggah',\n",
       " 'niggas',\n",
       " 'niggaz',\n",
       " 'nigger',\n",
       " 'niggers',\n",
       " 'no',\n",
       " 'nob',\n",
       " 'nobhead',\n",
       " 'nobjocky',\n",
       " 'nobjokey',\n",
       " 'nobody',\n",
       " 'non-white',\n",
       " 'nondescript',\n",
       " 'nonsense',\n",
       " 'not',\n",
       " 'noxious',\n",
       " 'numbnuts',\n",
       " 'nutsack',\n",
       " 'omg',\n",
       " 'orgasim',\n",
       " 'orgasims',\n",
       " 'orgasm',\n",
       " 'orgasms',\n",
       " 'oriental',\n",
       " 'p0rn',\n",
       " 'pakeha',\n",
       " 'paki',\n",
       " 'panooch',\n",
       " 'pawn',\n",
       " 'pecker',\n",
       " 'peckerhead',\n",
       " 'penis',\n",
       " 'penisbanger',\n",
       " 'penisfucker',\n",
       " 'penispuffer',\n",
       " 'phonesex',\n",
       " 'phuck',\n",
       " 'phuk',\n",
       " 'phuked',\n",
       " 'phuking',\n",
       " 'phukked',\n",
       " 'phukking',\n",
       " 'phuks',\n",
       " 'phuq',\n",
       " 'pickaninny',\n",
       " 'pigfucker',\n",
       " 'pimpis',\n",
       " 'piss',\n",
       " 'pissed',\n",
       " 'pisser',\n",
       " 'pissers',\n",
       " 'pisses',\n",
       " 'pissflaps',\n",
       " 'pissin',\n",
       " 'pissing',\n",
       " 'pissoff',\n",
       " 'polesmoker',\n",
       " 'pollock',\n",
       " 'pom',\n",
       " 'pommy',\n",
       " 'poon',\n",
       " 'poonani',\n",
       " 'poonany',\n",
       " 'poontang',\n",
       " 'poop',\n",
       " 'porchmonkey',\n",
       " 'porn',\n",
       " 'porno',\n",
       " 'pornography',\n",
       " 'pornos',\n",
       " 'prick',\n",
       " 'pricks',\n",
       " 'pron',\n",
       " 'pube',\n",
       " 'punanny',\n",
       " 'punta',\n",
       " 'pusse',\n",
       " 'pussi',\n",
       " 'pussies',\n",
       " 'pussy',\n",
       " 'pussylicking',\n",
       " 'pussys',\n",
       " 'puto',\n",
       " 'queaf',\n",
       " 'queer',\n",
       " 'rectum',\n",
       " 'redskin',\n",
       " 'retard',\n",
       " 'rimjaw',\n",
       " 'rimming',\n",
       " 's.o.b.',\n",
       " 's_h_i_t',\n",
       " 'sad',\n",
       " 'sadism',\n",
       " 'sadist',\n",
       " 'sandbar',\n",
       " 'savage',\n",
       " 'scare',\n",
       " 'scary',\n",
       " 'schlong',\n",
       " 'scream',\n",
       " 'screwing',\n",
       " 'scroat',\n",
       " 'scrote',\n",
       " 'scrotum',\n",
       " 'semen',\n",
       " 'severe',\n",
       " 'sex',\n",
       " 'sh!+',\n",
       " 'sh!t',\n",
       " 'sh1t',\n",
       " 'shag',\n",
       " 'shagger',\n",
       " 'shaggin',\n",
       " 'shagging',\n",
       " 'shemale',\n",
       " 'shi+',\n",
       " 'shit',\n",
       " 'shitdick',\n",
       " 'shite',\n",
       " 'shited',\n",
       " 'shitey',\n",
       " 'shitfuck',\n",
       " 'shitfull',\n",
       " 'shithead',\n",
       " 'shiting',\n",
       " 'shitings',\n",
       " 'shits',\n",
       " 'shitted',\n",
       " 'shitter',\n",
       " 'shitters',\n",
       " 'shitting',\n",
       " 'shittings',\n",
       " 'shitty',\n",
       " 'shocking',\n",
       " 'shoddy',\n",
       " 'sick',\n",
       " 'sickening',\n",
       " 'sinister',\n",
       " 'skank',\n",
       " 'slimy',\n",
       " 'slope',\n",
       " 'slut',\n",
       " 'sluts',\n",
       " 'smegma',\n",
       " 'smelly',\n",
       " 'smut',\n",
       " 'snatch',\n",
       " 'sobbing',\n",
       " 'son-of-a-bitch',\n",
       " 'sorry',\n",
       " 'spac',\n",
       " 'spade',\n",
       " 'spiteful',\n",
       " 'spunk',\n",
       " 'squaw',\n",
       " 'sticky',\n",
       " 'stinky',\n",
       " 'stormy',\n",
       " 'stressful',\n",
       " 'stuck',\n",
       " 'stupid',\n",
       " 'substandard',\n",
       " 'suck',\n",
       " 'suspect',\n",
       " 'suspicious',\n",
       " 't1tt1e5',\n",
       " 't1tties',\n",
       " 'teets',\n",
       " 'teez',\n",
       " 'testical',\n",
       " 'testicle',\n",
       " 'tit',\n",
       " 'titfuck',\n",
       " 'tits',\n",
       " 'titt',\n",
       " 'tittie5',\n",
       " 'tittiefucker',\n",
       " 'titties',\n",
       " 'tittyfuck',\n",
       " 'tittywank',\n",
       " 'titwank',\n",
       " 'tosser',\n",
       " 'turd',\n",
       " 'tw4t',\n",
       " 'twat',\n",
       " 'twathead',\n",
       " 'twatty',\n",
       " 'twunt',\n",
       " 'twunter',\n",
       " 'ugly',\n",
       " 'undermine',\n",
       " 'unfair',\n",
       " 'unfavorable',\n",
       " 'unhappy',\n",
       " 'unhealthy',\n",
       " 'unjust',\n",
       " 'unlucky',\n",
       " 'unpleasant',\n",
       " 'unsatisfactory',\n",
       " 'unsightly',\n",
       " 'untoward',\n",
       " 'unwanted',\n",
       " 'unwelcome',\n",
       " 'unwholesome',\n",
       " 'unwieldy',\n",
       " 'unwise',\n",
       " 'upset',\n",
       " 'v14gra',\n",
       " 'v1gra',\n",
       " 'vagina',\n",
       " 'viagra',\n",
       " 'vulva',\n",
       " 'w00se',\n",
       " 'wang',\n",
       " 'wank',\n",
       " 'wanker',\n",
       " 'wanky',\n",
       " 'waspy',\n",
       " 'wetback',\n",
       " 'whitey',\n",
       " 'whoar',\n",
       " 'whore',\n",
       " 'willies',\n",
       " 'willy',\n",
       " 'wog',\n",
       " 'wop',\n",
       " 'wtf',\n",
       " 'xrated',\n",
       " 'xxx',\n",
       " 'yid'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Actor-Critic Dialogue Model \n",
    "\n",
    "**! (symbolic expressions start from here)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "GRAD_CLIP = 5\n",
    "N_LSTM_UNITS = 1024\n",
    "EMB_SIZE = 512\n",
    "BOTTLENECK_UNITS = 256\n",
    "\n",
    "\n",
    "TEMPERATURE = theano.shared(np.float32(1.), name='temperature')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder\n",
    "\n",
    "* Just convolves sequence of input words into final hidden vector (so outputs [batch_size, N_LSTM_UNITS])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pretrained_network.wrong_lstm_layer import WrongLSTMLayer\n",
    "class Enc:\n",
    "    ### THEANO GRAPH INPUT ###\n",
    "    input_phrase = T.imatrix(\"encoder phrase tokens\")\n",
    "    ##########################\n",
    "    \n",
    "    l_in = InputLayer((None, None), input_phrase, name='context input')\n",
    "    l_mask = InputLayer((None, None), T.neq(input_phrase, PAD_ix), name='context mask')\n",
    "    \n",
    "    l_emb = EmbeddingLayer(l_in, N_TOKENS, EMB_SIZE, name=\"context embedding\")\n",
    "    \n",
    "    \n",
    "    ####LSTMLayer with incorrect outputgate####\n",
    "    \n",
    "    l_lstm = LSTMLayer(\n",
    "                        l_emb,\n",
    "                        N_LSTM_UNITS,\n",
    "                        name='encoder_lstm',\n",
    "                        grad_clipping=GRAD_CLIP,\n",
    "                        mask_input=l_mask,\n",
    "                        only_return_final=True,\n",
    "                        peepholes=False)\n",
    "    \n",
    "    weights = get_all_params(l_lstm, trainable=True)\n",
    "    \n",
    "    output = l_lstm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This defines one step of decoder.\n",
    "\n",
    "* Decoder takes next things as input (at each tick!): ``(prev_cell, prev_hid, inp_word, encoder_output)``\n",
    "* Decoder makes computations and output: ``(next_cell, next_hid, next_word)`` which will be passed as inputs at the next tick."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import theano.tensor as T\n",
    "import theano.tensor.shared_randomstreams as random_streams\n",
    "\n",
    "from agentnet.resolver.base import BaseResolver\n",
    "\n",
    "\n",
    "class ProbabilisticResolver(BaseResolver):\n",
    "    def __init__(self, incoming, assume_normalized=False, exclude_words_mask=None, seed=1234, output_dtype='int32',\n",
    "                 name='ProbabilisticResolver'):\n",
    "        self.assume_normalized = assume_normalized\n",
    "\n",
    "        self.rng = random_streams.RandomStreams(seed)\n",
    "        \n",
    "        self.exclude_words_mask = exclude_words_mask\n",
    "\n",
    "        super(ProbabilisticResolver, self).__init__(incoming, name=name,output_dtype=output_dtype)\n",
    "\n",
    "    def get_output_for(self, policy, greedy=False, **kwargs):\n",
    "        if self.exclude_words_mask:\n",
    "            assert self.exclude_words_mask.ndim == 1 \n",
    "            policy = policy*self.exclude_words_mask\n",
    "            self.assume_normalized = False\n",
    "\n",
    "        if greedy:\n",
    "            # greedy branch\n",
    "            chosen_action_ids = T.argmax(policy, axis=-1).astype(self.output_dtype)\n",
    "\n",
    "        else:\n",
    "            # probabilistic branch\n",
    "            batch_size, n_actions = policy.shape\n",
    "\n",
    "            if self.assume_normalized:\n",
    "                probas = policy\n",
    "            else:\n",
    "                probas = policy / T.sum(policy, axis=1, keepdims=True)\n",
    "\n",
    "            # p1, p1+p2, p1+p2+p3, ... 1\n",
    "            cum_probas = T.cumsum(probas, axis=1)\n",
    "\n",
    "            batch_randomness = self.rng.uniform(low=0., high=1., size=[batch_size, 1])\n",
    "\n",
    "            chosen_action_ids = T.sum((batch_randomness > cum_probas[:, :-1]), axis=1, dtype=self.output_dtype)\n",
    "\n",
    "        return chosen_action_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pretrained_network.wrong_lstm_cell import WrongLSTMCell\n",
    "from agentnet import Recurrence\n",
    "# from agentnet.resolver import  ProbabilisticResolver\n",
    "from agentnet.memory import LSTMCell\n",
    "\n",
    "class Dec:\n",
    "    # Define inputs of decoder at each time step.\n",
    "    prev_cell = InputLayer((None, N_LSTM_UNITS), name='cell')\n",
    "    prev_hid = InputLayer((None, N_LSTM_UNITS), name='hid')\n",
    "    input_word = InputLayer((None,))\n",
    "    encoder_lstm = InputLayer((None, N_LSTM_UNITS), name='encoder')\n",
    "\n",
    "    \n",
    "    # Embed input word and use the same embeddings as in the encoder.\n",
    "    word_embedding = EmbeddingLayer(input_word, N_TOKENS, EMB_SIZE,\n",
    "                                    W=Enc.l_emb.W, name='emb')\n",
    "    \n",
    "    \n",
    "    # This is not WrongLSTMLayer! *Cell is used for one-tick networks.\n",
    "    new_cell, new_hid = LSTMCell(prev_cell, prev_hid,\n",
    "                                      input_or_inputs=[word_embedding, encoder_lstm],\n",
    "                                      name='decoder_lstm',\n",
    "                                      peepholes=False)\n",
    "    \n",
    "    # Define parts for new word prediction. Bottleneck is a hack for reducing time complexity.\n",
    "    bottleneck = DenseLayer(new_hid, BOTTLENECK_UNITS, nonlinearity=T.tanh, name='decoder intermediate')\n",
    "\n",
    "    \n",
    "    next_word_probs = DenseLayer(bottleneck, N_TOKENS,\n",
    "                                 nonlinearity=lambda probs: T.nnet.softmax(probs/TEMPERATURE),\n",
    "                                 name='decoder next word probas')\n",
    "\n",
    "    next_words = ProbabilisticResolver(next_word_probs, assume_normalized=True)\n",
    "    \n",
    "    next_words_without_targets = ProbabilisticResolver(next_word_probs, assume_normalized=True,\n",
    "                                                       exclude_words_mask=target_idxs_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generator unrolls decoder for ``n_steps``\n",
    "\n",
    "* It uses ``Recurrence`` class from ``agentnet``.\n",
    "\n",
    "* ``theano.scan`` fn takes arguments in order: ``seq1, seq2,..,output1,output2,..,nonseq1,nonseq2,...``\n",
    "* ``Recurrence`` unrolls ``Dec`` for `n_steps`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class Gen:\n",
    "    n_steps = theano.shared(25)\n",
    "    # This theano tensor is used as first input word for decoder.\n",
    "    bos_input_var = T.zeros((Enc.input_phrase.shape[0],), 'int32')+BOS_ix\n",
    "    \n",
    "    bos_input_layer = InputLayer((None,), bos_input_var, name=\"first input\")\n",
    "\n",
    "    recurrence = Recurrence(\n",
    "        # This means that encoder.output passed to decoder.encoder_lstm input at each tick.\n",
    "        input_nonsequences={Dec.encoder_lstm: Enc.output},\n",
    "        \n",
    "        # This defines how outputs moves to inputs at each tick in decoder. \n",
    "        # These corresponds to outputs in theano scan function.\n",
    "        state_variables=OrderedDict([(Dec.new_cell, Dec.prev_cell),\n",
    "                                     (Dec.new_hid, Dec.prev_hid),\n",
    "                                     (Dec.next_words, Dec.input_word)]),\n",
    "        # We will need these probabilities for Actor-Critic algorithm.\n",
    "        tracked_outputs=[Dec.next_word_probs, Dec.next_words_without_targets],\n",
    "        state_init={Dec.next_words: bos_input_layer},\n",
    "        n_steps=n_steps,\n",
    "        unroll_scan=False)\n",
    "    \n",
    "    recurrence_outputs = get_output(recurrence)\n",
    "        \n",
    "    ##### DECODER UNROLLED #####\n",
    "    # Theano tensor which represents sequence of generated words and their probabilities.\n",
    "    words_seq = recurrence_outputs[Dec.next_words]\n",
    "    words_probs_seq = recurrence_outputs[Dec.next_word_probs]\n",
    "    words_probs_seq_without_target = words_probs_seq*target_idxs_mask[None, None, :]\n",
    "    words_seq_without_target = recurrence_outputs[Dec.next_words_without_targets]\n",
    "    \n",
    "    # Theano tensor which represents decoder hidden states.\n",
    "    dec_cell_seq = recurrence_outputs[Dec.new_cell]\n",
    "    dec_hid_seq = recurrence_outputs[Dec.new_hid]\n",
    "    ############################\n",
    "                                                 \n",
    "    generate = theano.function([Enc.input_phrase], [words_seq, dec_cell_seq, dec_hid_seq],\n",
    "                               updates=recurrence.get_automatic_updates())\n",
    "    \n",
    "    generate_without_target = theano.function([Enc.input_phrase], [words_seq_without_target, dec_cell_seq, dec_hid_seq],\n",
    "                                              updates=recurrence.get_automatic_updates())\n",
    "    \n",
    "    weights = get_all_params(recurrence, trainable=True)\n",
    "    \n",
    "    @staticmethod\n",
    "    def reply(phrase, max_len=25, **kwargs):\n",
    "        old_value = Gen.n_steps.get_value()\n",
    "        \n",
    "        Gen.n_steps.set_value(max_len)\n",
    "        phrase_ix = phrase2matrix([phrase],**kwargs)\n",
    "        answer_ix = Gen.generate(phrase_ix)[0][0]\n",
    "        if EOS_ix in answer_ix:\n",
    "            answer_ix = answer_ix[:list(answer_ix).index(EOS_ix)]\n",
    "            \n",
    "        Gen.n_steps.set_value(old_value)\n",
    "        return ' '.join(map(tokens.__getitem__, answer_ix))\n",
    "        \n",
    "class GenTrain:\n",
    "    \"\"\"contains a recurrent loop where network is fed with reference answers instead of her own outputs.\n",
    "    Also contains some functions that train network in that mode.\"\"\"\n",
    "    \n",
    "    ### THEANO GRAPH INPUT. ###\n",
    "    reference_answers = T.imatrix(\"decoder reference answers\") # shape [batch_size, max_len]\n",
    "    ###########################\n",
    "    \n",
    "    bos_column = T.zeros((reference_answers.shape[0], 1), 'int32')+BOS_ix\n",
    "    reference_answers_bos = T.concatenate((bos_column, reference_answers), axis=1)  #prepend BOS\n",
    "    \n",
    "    l_ref_answers = InputLayer((None, None), reference_answers_bos, name='context input')\n",
    "    l_ref_mask = InputLayer((None, None), T.neq(reference_answers_bos, PAD_ix), name='context mask')\n",
    "    \n",
    "    recurrence = Recurrence(\n",
    "        input_nonsequences=OrderedDict([(Dec.encoder_lstm, Enc.output)]),\n",
    "        input_sequences=OrderedDict([(Dec.input_word, l_ref_answers)]),\n",
    "        state_variables=OrderedDict([(Dec.new_cell, Dec.prev_cell),\n",
    "                                     (Dec.new_hid, Dec.prev_hid)]),\n",
    "        tracked_outputs=[Dec.next_word_probs, Dec.next_words],\n",
    "        mask_input=l_ref_mask,\n",
    "        unroll_scan=False)\n",
    "    \n",
    "    recurrence_outputs = get_output(recurrence)\n",
    "    \n",
    "    P_seq = recurrence_outputs[Dec.next_word_probs]\n",
    "    \n",
    "    \n",
    "    ############################\n",
    "    ###loglikelihood training###\n",
    "    ############################\n",
    "    predicted_probas = P_seq[:, :-1].reshape((-1, N_TOKENS))+1e-6\n",
    "    target_labels = reference_answers.ravel()\n",
    "    \n",
    "    llh_loss = lasagne.objectives.categorical_crossentropy(predicted_probas, target_labels).mean()\n",
    "    \n",
    "    llh_updates = lasagne.updates.adam(llh_loss, Gen.weights, 0.001)\n",
    "    \n",
    "    train_step = theano.function([Enc.input_phrase, reference_answers], llh_loss,\n",
    "                                 updates=llh_updates+recurrence.get_automatic_updates())\n",
    "    get_llh = theano.function([Enc.input_phrase, reference_answers], llh_loss, no_default_updates=True)\n",
    "    \n",
    "    sample_probs = theano.function([Enc.input_phrase, reference_answers], P_seq, no_default_updates=True)\n",
    "    get_llh_from_sampled = theano.function([P_seq, reference_answers], llh_loss, no_default_updates=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "# Actor-Critic part"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Critic\n",
    "\n",
    "* Critic uses decoder hidden vectors as state.\n",
    "* Critic evaluates state values for `[hid_t, cell_t]` before predicting `word_t`.\n",
    "* As a reward for being in state `[hid_t, cell_t]` and generating word `word_t` we take 1 if word `word_t` is obscene, 0 otherwise.\n",
    "\n",
    "* During training, we can compute these rewards trivially (we just need to collect a dictionary of obscene words).\n",
    "* We use TD updates $V(s) \\leftarrow V(s) + \\alpha * (R + V(s') - V(s))$; for function approximation, its equivalent to ``new_weights = old_weights - alpha * grad(MSE(targetV, approxV), old_weights)``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from agentnet.learning.generic import get_n_step_value_reference\n",
    "from theano.gradient import disconnected_grad\n",
    "\n",
    "class Critic:\n",
    "    rewards, is_alive = _calc_rewards(Gen.words_seq)  # both with shape [batch_size, n_steps]\n",
    "    rewards = -1 * rewards ## PREVENT BAD WORDS\n",
    "    \n",
    "    critic_input_var = Gen.dec_hid_seq\n",
    "    \n",
    "    l_dec_cell_seq = InputLayer((None, None, N_LSTM_UNITS), input_var=critic_input_var, name='l_decoder_sequence')\n",
    "    \n",
    "    l_critic_values = DenseLayer(l_dec_cell_seq, num_units=512, num_leading_axes=2, name=\"critic_dense1\")\n",
    "    l_critic_values = DenseLayer(l_critic_values, num_units=512, num_leading_axes=2, name=\"critic_dense2\")\n",
    "    l_critic_values = DenseLayer(l_critic_values, num_units=256, num_leading_axes=2, name=\"critic_dense3\")\n",
    "    l_critic_values = DenseLayer(l_critic_values, num_units=1, num_leading_axes=2, \n",
    "                                 nonlinearity=None, name='critic_values')\n",
    "    \n",
    "    critic_values_seq = get_output(l_critic_values)\n",
    "    # Now its shape [batch, n_steps, 1]. Reshape it to [batch, n_steps]\n",
    "    _old_shape = critic_values_seq.shape\n",
    "    critic_values_seq = critic_values_seq.reshape((_old_shape[0], _old_shape[1]))\n",
    "    \n",
    "    predict = theano.function([Enc.input_phrase], [Gen.words_seq, critic_values_seq, rewards, is_alive],\n",
    "                              allow_input_downcast=True, no_default_updates=True)\n",
    "    \n",
    "    predict_values_from_decoder = theano.function([Gen.dec_hid_seq], critic_values_seq,\n",
    "                                                  allow_input_downcast=True, no_default_updates=True)\n",
    "    \n",
    "    weights = get_all_params(l_critic_values, trainable=True)\n",
    "\n",
    "class CriticTrainer:\n",
    "    td_n_steps = 1\n",
    "    \n",
    "    V_predicted = Critic.critic_values_seq\n",
    "    \n",
    "    V_reference = get_n_step_value_reference(state_values=V_predicted,\n",
    "                                             rewards=Critic.rewards,\n",
    "                                             is_alive=Critic.is_alive,\n",
    "                                             n_steps=td_n_steps)\n",
    "    \n",
    "    # We must not propagate grads through target value (semi-gradient method).\n",
    "    V_reference = disconnected_grad(V_reference)\n",
    "    \n",
    "    td_loss = lasagne.objectives.squared_error(V_predicted, V_reference).sum(axis=1).mean()\n",
    "    td_updates = lasagne.updates.adam(td_loss, Critic.weights)\n",
    "    \n",
    "    train_step = theano.function([Enc.input_phrase], [td_loss, V_predicted, V_reference, Critic.rewards, Critic.is_alive],\n",
    "                                 updates=td_updates+Gen.recurrence.get_automatic_updates(),\n",
    "                                 allow_input_downcast=True)\n",
    "    \n",
    "    \n",
    "CT = CriticTrainer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Actor\n",
    "\n",
    "Actor need to update policy (which is defined by `Gen`) using policy gradient: $\\nabla\\log\\pi(a|s)\\cdot A(s,a)$, where $A(s,a)$ is the advantage function of being in state $s$ and doing action $a$. In our case, $A(s,a) = R_1+\\gamma R_2+\\dots+\\gamma^nV(s')-V(s)$\n",
    "\n",
    "In code, the scheme looks like this:\n",
    "* Take __generated batch__ (actions, needed for computing rewards), corresponding __`dec_cell_seq, dec_hid_seq`__ (states, needed for computing advantage function) and __probabilities of words__ on each tick (policy, we will update it).\n",
    "* Calculate `rewards` and `is_alive` mask for this batch.\n",
    "* Calculate state-values using critic and target values (n_steps TD estimation of Q-function, e.g. `Critic.V_reference`). Compute advantage function and disconnect gradient through it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ActorTrainer:\n",
    "    LLH_ALPHA = theano.shared(5.0)\n",
    "    \n",
    "    actions = Gen.words_seq  # shape [batch_size, n_steps]\n",
    "    _actions_ravel = actions.ravel()  # shape [batch_size*n_steps,]\n",
    "    \n",
    "    _word_probs = Gen.words_probs_seq\n",
    "    _old_shape = _word_probs.shape\n",
    "    _word_probs = _word_probs.reshape((-1, _old_shape[-1]))  # shape [batch_size*n_steps, vocab_size]\n",
    "    \n",
    "    _policy = _word_probs[T.arange(_word_probs.shape[0]), _actions_ravel]  # shape [batch_size*n_steps,]\n",
    "    \n",
    "    policy = _policy.reshape((_old_shape[0], _old_shape[1])) # shape [batch_size, n_steps]\n",
    "    advantage = CriticTrainer.V_reference-CriticTrainer.V_predicted\n",
    "    \n",
    "    pg_loss = (-T.log(policy) * disconnected_grad(advantage)).sum(axis=1).mean()\n",
    "    llh_loss = GenTrain.llh_loss\n",
    "    \n",
    "    loss = pg_loss + LLH_ALPHA * llh_loss\n",
    "    \n",
    "#     actor_weights = [param for param in Gen.weights if param not in Enc.weights]\n",
    "    actor_weights = actor_weights = [param for param in Gen.weights]\n",
    "    \n",
    "    grads = T.grad(loss, actor_weights)\n",
    "    grads = lasagne.updates.total_norm_constraint(grads, GRAD_CLIP)\n",
    "    \n",
    "    policy_updates = lasagne.updates.adam(grads, actor_weights)\n",
    "    \n",
    "    train_step = theano.function([Enc.input_phrase, GenTrain.reference_answers],\n",
    "                                 [pg_loss, llh_loss, policy, actions, advantage, Critic.rewards, Critic.is_alive],\n",
    "                                 updates=policy_updates+GenTrain.recurrence.get_automatic_updates()+Gen.recurrence.get_automatic_updates())\n",
    "    \n",
    "AT = ActorTrainer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## How to train Critic\n",
    "\n",
    "__How do we train critic?__\n",
    "* Iterating over batches of data (it will be `Enc.input_phrase`)\n",
    "* Call `CT.train_step(batch)`.\n",
    "\n",
    "That's it! Simple.\n",
    "\n",
    "\n",
    "## How to train Actor-Critic\n",
    "\n",
    "**Training both actor-critic with policy gradient**:\n",
    "\n",
    "* Iterating over batches of data.\n",
    "* Call `loss, policy, actions, advantage, rewards, is_alive = AT.train_step(batch_context, batch_answers)`\n",
    "* Call `td_loss, _, _, _, _ = CT.train_step(batch_context)`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Actor-Critic [look at bepolite-experiments.ipynb]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
