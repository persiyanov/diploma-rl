{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from mymodule.data_stuff import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "import pickle\n",
    "\n",
    "def read_all_contexts(context_size=2, verbose=100000):\n",
    "    with open('./open_subtitles_en_raw') as fin:\n",
    "        lines = []\n",
    "        for l in fin:\n",
    "            lines.append(l.strip())\n",
    "    contexts = []\n",
    "    curr_context = deque(lines[:context_size], context_size)\n",
    "    curr_answer = lines[context_size]\n",
    "    \n",
    "    t = 0\n",
    "    for line in lines[context_size+1:]:\n",
    "        contexts.append({'context':list(curr_context), 'answer': curr_answer})\n",
    "\n",
    "        if t % verbose == 0:\n",
    "            print(t)\n",
    "        curr_context.append(curr_answer)\n",
    "        curr_answer = line.strip()\n",
    "\n",
    "        t += 1\n",
    "    return contexts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open('contexts.pkl', 'rb') as fin:\n",
    "    contexts = pickle.load(fin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 18845078/18845078 [30:02<00:00, 10453.51it/s]\n"
     ]
    }
   ],
   "source": [
    "maxlen = 0\n",
    "for i in tqdm.trange(len(contexts)):\n",
    "    maxlen = max(len(preprocess(contexts[i]['context'])), maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "Missing parentheses in call to 'print' (<ipython-input-8-a0bf52e124cb>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-8-a0bf52e124cb>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    print maxlen\u001b[0m\n\u001b[0m               ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m Missing parentheses in call to 'print'\n"
     ]
    }
   ],
   "source": [
    "print maxlen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.hist(lens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'answer': 'Hey, Nick.',\n",
       " 'context': ['Kids can get pretty much anything they want in the yard, as long as they can afford it.',\n",
       "  \"'Cause everything comes with a price.\"]}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "contexts[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Actor-Critic Dialogue Model \n",
    "\n",
    "**! (symbolic expressions start from here)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: THEANO_FLAGS=device=gpu4,floatX=float32,exception_verbosity=high,lib.cnmem=0.8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using gpu device 4: GeForce GTX 1080 (CNMeM is enabled with initial size: 80.0% of memory, cuDNN 5105)\n"
     ]
    }
   ],
   "source": [
    "%env THEANO_FLAGS=device=gpu4,floatX=float32,exception_verbosity=high,lib.cnmem=0.8\n",
    "from warnings import warn\n",
    "import numpy as np\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "from collections import OrderedDict\n",
    "import lasagne"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "GRAD_CLIP = 5\n",
    "N_LSTM_UNITS = 1024\n",
    "EMB_SIZE = 512\n",
    "BOTTLENECK_UNITS = 256\n",
    "\n",
    "TEMPERATURE = theano.shared(np.float32(1.), name='temperature')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder\n",
    "\n",
    "* Just convolves sequence of input words into final hidden vector (so outputs [batch_size, N_LSTM_UNITS])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from lasagne.layers import *\n",
    "\n",
    "class Enc:\n",
    "    ### THEANO GRAPH INPUT ###\n",
    "    input_phrase = T.imatrix(\"encoder phrase tokens\")\n",
    "    ##########################\n",
    "    \n",
    "    l_in = InputLayer((None, None), input_phrase, name='context input')\n",
    "    l_mask = InputLayer((None, None), T.neq(input_phrase, PAD_ix), name='context mask')\n",
    "    \n",
    "    l_emb = EmbeddingLayer(l_in, N_TOKENS, EMB_SIZE, name=\"context embedding\")\n",
    "    \n",
    "    \n",
    "    ####LSTMLayer with CORRECT outputgate####\n",
    "    \n",
    "    l_lstm = LSTMLayer(l_emb,\n",
    "                       N_LSTM_UNITS,\n",
    "                       name='encoder_lstm',\n",
    "                       grad_clipping=GRAD_CLIP,\n",
    "                       mask_input=l_mask,\n",
    "                       only_return_final=True,\n",
    "                       peepholes=False)\n",
    "    \n",
    "    output = l_lstm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This defines one step of decoder.\n",
    "\n",
    "* Decoder takes next things as input (at each tick!): ``(prev_cell, prev_hid, inp_word, encoder_output)``\n",
    "* Decoder makes computations and output: ``(next_cell, next_hid, next_word)`` which will be passed as inputs at the next tick."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from agentnet import Recurrence\n",
    "from agentnet.resolver import  ProbabilisticResolver\n",
    "from agentnet.memory import LSTMCell\n",
    "\n",
    "class Dec:\n",
    "    # Define inputs of decoder at each time step.\n",
    "    prev_cell = InputLayer((None, N_LSTM_UNITS), name='cell')\n",
    "    prev_hid = InputLayer((None, N_LSTM_UNITS), name='hid')\n",
    "    input_word = InputLayer((None,))\n",
    "    encoder_lstm = InputLayer((None, N_LSTM_UNITS), name='encoder')\n",
    "\n",
    "    \n",
    "    # Embed input word and use the same embeddings as in the encoder.\n",
    "    word_embedding = EmbeddingLayer(input_word, N_TOKENS, EMB_SIZE,\n",
    "                                    W=Enc.l_emb.W, name='emb')\n",
    "    \n",
    "    \n",
    "    # This is not WrongLSTMLayer! *Cell is used for one-tick networks.\n",
    "    new_cell, new_hid = LSTMCell(prev_cell, prev_hid,\n",
    "                                 input_or_inputs=[word_embedding, encoder_lstm],\n",
    "                                 name='decoder_lstm',\n",
    "                                 peepholes=False)\n",
    "    \n",
    "    # Define parts for new word prediction. Bottleneck is a hack for reducing time complexity.\n",
    "    bottleneck = DenseLayer(new_hid, BOTTLENECK_UNITS, nonlinearity=T.tanh, name='decoder intermediate')\n",
    "\n",
    "    \n",
    "    next_word_probs = DenseLayer(bottleneck, N_TOKENS,\n",
    "                                 nonlinearity=lambda probs: T.nnet.softmax(probs/TEMPERATURE),\n",
    "                                 name='decoder next word probas')\n",
    "\n",
    "    next_words = ProbabilisticResolver(next_word_probs, assume_normalized=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generator unrolls decoder for ``n_steps``\n",
    "\n",
    "* It uses ``Recurrence`` class from ``agentnet``.\n",
    "\n",
    "* ``theano.scan`` fn takes arguments in order: ``seq1, seq2,..,output1,output2,..,nonseq1,nonseq2,...``\n",
    "* ``Recurrence`` unrolls ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hedgedir/agentnet/agentnet/agent/recurrence.py:595: UserWarning: Warning: recurrent loop without unroll_scan got nonempty random state updates list. That happened because there is some source of randomness (e.g. dropout) inside recurrent step graph. To compile such graph, one must either call .get_automatic_updates() right after .get_output and pass these updates to a function, or use no_defalt_updates=True when compiling theano.function.\n",
      "  warn(\"Warning: recurrent loop without unroll_scan got nonempty random state updates list. That happened\"\n"
     ]
    }
   ],
   "source": [
    "class GenTest:\n",
    "    n_steps = theano.shared(25)\n",
    "    # This theano tensor is used as first input word for decoder.\n",
    "    bos_input_var = T.zeros((Enc.input_phrase.shape[0],), 'int32')+BOS_ix\n",
    "    \n",
    "    bos_input_layer = InputLayer((None,), bos_input_var, name=\"first input\")\n",
    "\n",
    "    recurrence = Recurrence(\n",
    "        # This means that encoder.output passed to decoder.encoder_lstm input at each tick.\n",
    "        input_nonsequences={Dec.encoder_lstm: Enc.output},\n",
    "        \n",
    "        # This defines how outputs moves to inputs at each tick in decoder. \n",
    "        # These corresponds to outputs in theano scan function.\n",
    "        state_variables=OrderedDict([(Dec.new_cell, Dec.prev_cell),\n",
    "                                     (Dec.new_hid, Dec.prev_hid),\n",
    "                                     (Dec.next_words, Dec.input_word)]),        \n",
    "        state_init={Dec.next_words: bos_input_layer},\n",
    "        n_steps=n_steps,\n",
    "        unroll_scan=False)\n",
    "    \n",
    "    weights = get_all_params(recurrence, trainable=True)\n",
    "    \n",
    "    recurrence_outputs = get_output(recurrence)\n",
    "        \n",
    "    ##### DECODER UNROLLED #####\n",
    "    # Theano tensor which represents sequence of generated words.\n",
    "    words_seq = recurrence_outputs[Dec.next_words]\n",
    "    \n",
    "    # Theano tensor which represents decoder hidden states.\n",
    "    dec_cell_seq = recurrence_outputs[Dec.new_cell]\n",
    "    ############################\n",
    "                                                 \n",
    "    generate = theano.function([Enc.input_phrase], [words_seq, dec_cell_seq],\n",
    "                               updates=recurrence.get_automatic_updates())\n",
    "    \n",
    "    \n",
    "    @staticmethod\n",
    "    def reply(phrase, max_len=25, **kwargs):\n",
    "        old_value = GenTest.n_steps.get_value()\n",
    "        \n",
    "        GenTest.n_steps.set_value(max_len)\n",
    "        phrase_ix = phrase2matrix([phrase],**kwargs)\n",
    "        answer_ix = GenTest.generate(phrase_ix)[0][0]\n",
    "        if EOS_ix in answer_ix:\n",
    "            answer_ix = answer_ix[:list(answer_ix).index(EOS_ix)]\n",
    "            \n",
    "        GenTest.n_steps.set_value(old_value)\n",
    "        return ' '.join(map(tokens.__getitem__, answer_ix))\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LM Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hedgedir/agentnet/agentnet/agent/recurrence.py:301: UserWarning: You are giving Recurrence an input sequence of undefined length (None).\n",
      "Make sure it is always above <unspecified>(n_steps) you specified for recurrence\n",
      "  \"Make sure it is always above {}(n_steps) you specified for recurrence\".format(n_steps or \"<unspecified>\"))\n",
      "/home/hedgedir/agentnet/agentnet/agent/recurrence.py:595: UserWarning: Warning: recurrent loop without unroll_scan got nonempty random state updates list. That happened because there is some source of randomness (e.g. dropout) inside recurrent step graph. To compile such graph, one must either call .get_automatic_updates() right after .get_output and pass these updates to a function, or use no_defalt_updates=True when compiling theano.function.\n",
      "  warn(\"Warning: recurrent loop without unroll_scan got nonempty random state updates list. That happened\"\n"
     ]
    }
   ],
   "source": [
    "class GenTrain:\n",
    "    \"\"\"contains a recurrent loop where network is fed with reference answers instead of her own outputs.\n",
    "    Also contains some functions that train network in that mode.\"\"\"\n",
    "    \n",
    "    ### THEANO GRAPH INPUT. ###\n",
    "    reference_answers = T.imatrix(\"decoder reference answers\") # shape [batch_size, max_len]\n",
    "    ###########################\n",
    "    \n",
    "    bos_column = T.zeros((reference_answers.shape[0], 1), 'int32')+BOS_ix\n",
    "    reference_answers_bos = T.concatenate((bos_column, reference_answers), axis=1)  #prepend BOS\n",
    "    \n",
    "    l_ref_answers = InputLayer((None, None), reference_answers_bos, name='context input')\n",
    "    l_ref_mask = InputLayer((None, None), T.neq(reference_answers_bos, PAD_ix), name='context mask')\n",
    "    \n",
    "    recurrence = Recurrence(\n",
    "        input_nonsequences=OrderedDict([(Dec.encoder_lstm, Enc.output)]),\n",
    "        input_sequences=OrderedDict([(Dec.input_word, l_ref_answers)]),\n",
    "        state_variables=OrderedDict([(Dec.new_cell, Dec.prev_cell),\n",
    "                                     (Dec.new_hid, Dec.prev_hid)]),\n",
    "        tracked_outputs=[Dec.next_word_probs, Dec.next_words],\n",
    "        mask_input=l_ref_mask,\n",
    "        unroll_scan=False)\n",
    "    \n",
    "    recurrence_outputs = get_output(recurrence)\n",
    "    \n",
    "    P_seq = recurrence_outputs[Dec.next_word_probs]\n",
    "    \n",
    "    \n",
    "    ############################\n",
    "    ###loglikelihood training###\n",
    "    ############################\n",
    "    predicted_probas = P_seq[:, :-1].reshape((-1, N_TOKENS))+1e-6\n",
    "    target_labels = reference_answers.ravel()\n",
    "    \n",
    "    llh_loss = lasagne.objectives.categorical_crossentropy(predicted_probas, target_labels).mean()\n",
    "    \n",
    "    llh_updates = lasagne.updates.adam(llh_loss, GenTest.weights, 0.001)\n",
    "    \n",
    "    train_step = theano.function([Enc.input_phrase, reference_answers], llh_loss,\n",
    "                                 updates=llh_updates+recurrence.get_automatic_updates())\n",
    "    get_llh = theano.function([Enc.input_phrase, reference_answers], llh_loss, no_default_updates=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from agentnet.utils.persistence import save,load\n",
    "# load(GenTest.recurrence,\"pretrained_network/weights.pcl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def iterate_minibatches(batch_size=64):\n",
    "    total_batches = len(contexts)//batch_size\n",
    "    for b in range(total_batches):\n",
    "        excerpt = contexts[b*batch_size:(b+1)*batch_size]\n",
    "        excerpt_context = [item['context'] for item in excerpt]\n",
    "        excerpt_answer = [item['answer'] for item in excerpt]\n",
    "        yield phrase2matrix(excerpt_context), phrase2matrix(excerpt_answer)\n",
    "        \n",
    "from IPython.display import clear_output\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "test_phrases = [['Hello! How are you?'],\n",
    "                ['How old are you?'],\n",
    "                ['Are you fucking kidding me?'],\n",
    "                ['Suck. What are you doing?'], \n",
    "                ['You are piece of shit!!!'], \n",
    "                ['holy fucking crap. you are motherfucker']]\n",
    "\n",
    "\n",
    "BATCH_SIZE = 128\n",
    "N_EPOCHS = 100\n",
    "VERBOSITY = 10 # number of batches before printing\n",
    "NUM_BATCHES = len(contexts)//BATCH_SIZE\n",
    "\n",
    "SAVE_EVERY = 500\n",
    "\n",
    "f_log = open('log.txt', 'w')\n",
    "WEIGHTS_FILE = 'weights/LM_small.pkl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<agentnet.agent.recurrence.Recurrence at 0x7f8dd9230048>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load(GenTest.recurrence, WEIGHTS_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "loss_history = []\n",
    "# with open('loss_history.pkl', 'rb') as fin:\n",
    "#     loss_history = pickle.load(fin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:\t8.5311\n"
     ]
    }
   ],
   "source": [
    "for n_epoch in range(N_EPOCHS):\n",
    "    for nb,batch in enumerate(iterate_minibatches(BATCH_SIZE)):\n",
    "        ## Saving stuff.\n",
    "        if (n_epoch*NUM_BATCHES + nb + 1) % SAVE_EVERY == 0:\n",
    "            save(GenTest.recurrence, WEIGHTS_FILE)\n",
    "            f_log.write(\"\\nSAVED WEIGHTS!!!\\n\")\n",
    "        \n",
    "        ## Printing stuff.\n",
    "        if (n_epoch*NUM_BATCHES + nb + 1) % VERBOSITY == 0:\n",
    "            clear_output(wait=True)\n",
    "            f_log.write(\"Processed {}/{} epochs and {}/{} batches in current epoch\\n\".format(n_epoch, N_EPOCHS,\n",
    "                                                                                     nb+1, NUM_BATCHES))\n",
    "            f_log.write(\"Loss (averaged with last 10 batches): {0:.5f}\\n\".format(np.mean(loss_history[-10:])))\n",
    "            print(\"Loss:\\t{:.4f}\".format(np.mean(loss_history[-10:])))\n",
    "            \n",
    "            f_log.write(\"Answers on test phrases:\\n\")\n",
    "\n",
    "            for i in range(len(test_phrases)):\n",
    "                f_log.write(\"Phrase:\\t{}\\n\".format(test_phrases[i]))\n",
    "                f_log.write(\"Answer:\\t{}\\n\".format(GenTest.reply(test_phrases[i])))\n",
    "                f_log.write('---'*5+'\\n')\n",
    "            f_log.write('****'*10+'\\n')\n",
    "            \n",
    "            f_log.flush()\n",
    "            \n",
    "            with open('loss_history.pkl', 'wb') as fout:\n",
    "                pickle.dump(loss_history, fout)\n",
    "        \n",
    "        ## Training stuff.\n",
    "        batch_loss = GenTrain.train_step(batch[0], batch[1])\n",
    "    \n",
    "        loss_history.append(batch_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Critic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Actor-Critic part**:\n",
    "* Decoder also tries to evaluate **V-function** at each tick. **State** is hidden cell of LSTM layer.\n",
    "* As a reward for being in state `s` and generating word `a` we take 1 if word `a` is obscene, 0 otherwise.\n",
    "\n",
    "* During training, we can compute these rewards trivially (we just need to collect a dictionary of obscene words).\n",
    "* We use TD updates [V(s) = V(s) + alpha \\* (R + V(s') - V(s))]; for function approximation, its equivalent to ``new_weights = old_weights - alpha * grad(MSE(targetV, approxV), old_weights)``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hedgedir/agentnet/agentnet/agent/recurrence.py:301: UserWarning: You are giving Recurrence an input sequence of undefined length (None).\n",
      "Make sure it is always above <TensorType(int64, scalar)>(n_steps) you specified for recurrence\n",
      "  \"Make sure it is always above {}(n_steps) you specified for recurrence\".format(n_steps or \"<unspecified>\"))\n"
     ]
    }
   ],
   "source": [
    "class Critic:\n",
    "    l_dec_cell = InputLayer((None, N_LSTM_UNITS), name='l_decoder_cell') # shape [batch_size, N_LSTM_UNITS]\n",
    "    \n",
    "    ### THEANO GRAPH INPUT. ###\n",
    "    dec_cell_seq = T.tensor3('decoder hidden cell sequence after recurrence') # shape [batch_size, n_steps, N_LSTM_UNITS]\n",
    "    ###########################\n",
    "    \n",
    "    l_dec_cell_seq = InputLayer((None, None, N_LSTM_UNITS), input_var=dec_cell_seq, name='l_decoder_cell_sequence')\n",
    "    \n",
    "    l_critic_values = DenseLayer(l_dec_cell, num_units=512, name=\"CRITIC__dense1\")\n",
    "    l_critic_values = DenseLayer(l_critic_values, num_units=256, name=\"CRITIC_dense2\")\n",
    "    l_critic_values = DenseLayer(l_critic_values, num_units=1, nonlinearity=None, name='CRITIC__values')\n",
    "    \n",
    "    recurrence = Recurrence(\n",
    "        input_sequences={l_dec_cell: l_dec_cell_seq},\n",
    "        tracked_outputs=[l_critic_values],\n",
    "        unroll_scan=False,\n",
    "        n_steps=GenTest.n_steps\n",
    "    )\n",
    "    \n",
    "    rec_outputs = get_output(recurrence)\n",
    "    \n",
    "    critic_values_seq = rec_outputs[l_critic_values]\n",
    "    # Now its shape [batch, n_steps, 1]. Reshape it to [batch, n_steps]\n",
    "    old_shape = critic_values_seq.shape\n",
    "    critic_values_seq = critic_values_seq.reshape((old_shape[0], old_shape[1]))\n",
    "    \n",
    "    predict = theano.function([dec_cell_seq], critic_values_seq, allow_input_downcast=True)\n",
    "    \n",
    "    weights = get_all_params(l_critic_values, trainable=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from agentnet.learning.generic import get_n_step_value_reference\n",
    "from theano.gradient import disconnected_grad\n",
    "\n",
    "class CriticTrainer:\n",
    "    td_n_steps = 3\n",
    "    \n",
    "    # We will obtain these values from calc_rewards() function.\n",
    "    rewards = T.matrix(name='rewards for each generated word') # shape [batch_size, n_steps]\n",
    "    is_alive = T.imatrix(name='is alive mask') # shape [batch_size, n_steps]\n",
    "    \n",
    "    V_predicted = Critic.critic_values_seq\n",
    "    \n",
    "    V_reference = get_n_step_value_reference(state_values=V_predicted,\n",
    "                                             rewards=rewards,\n",
    "                                             is_alive=is_alive,\n",
    "                                             n_steps=td_n_steps)\n",
    "    \n",
    "    # We must not propagate grads through target value (semi-gradient method).\n",
    "    V_reference = disconnected_grad(V_reference)\n",
    "    \n",
    "    td_loss = lasagne.objectives.squared_error(V_predicted, V_reference).mean()\n",
    "    td_updates = lasagne.updates.adam(td_loss, Critic.weights)\n",
    "    \n",
    "    train_step = theano.function([rewards, is_alive, Critic.dec_cell_seq], td_loss,\n",
    "                                 updates=td_updates,\n",
    "                                 allow_input_downcast=True)\n",
    "    \n",
    "    \n",
    "CT = CriticTrainer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Critic training\n",
    "\n",
    "__How do we train critic?__\n",
    "* Iterating over batches of data (it will be `Enc.input_phrase`)\n",
    "* Generating `GenTest.words_seq, GenTest.dec_cell_seq`.\n",
    "* Computing rewards and is_alive mask using `GenTest.words_seq`\n",
    "* Propagating gradients to critic calling `CT.train_step(rewards, is_alive, dec_cell_seq)`.\n",
    "\n",
    "That's it! Simple."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Here we define how rewards are calculated based on generated sequences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Let's load obscene words list first__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Some examples ['bitch', 'niggaz', 'shitdick', 'penis', 'sh!t', 'fuckwhit', 'knobead', 'nigger', 'nobjocky', 'pissin']\n",
      "Number of target words 154\n"
     ]
    }
   ],
   "source": [
    "with open('./obscene_words.txt') as fin:\n",
    "    target_words = set()\n",
    "    for line in fin:\n",
    "        words_in_line = line.strip().split()\n",
    "        if len(words_in_line) == 1:\n",
    "            target_words.update(words_in_line)\n",
    "            \n",
    "target_words.add('suck')\n",
    "target_words.add('goddamn')\n",
    "target_words.add('motherfucker')\n",
    "target_words.add('nigger')\n",
    "target_words.add('nigga')\n",
    "target_words.add('ass')\n",
    "target_words.add('crap')\n",
    "target_words.add('fucking')\n",
    "\n",
    "print(\"Some examples %s\" % list(target_words)[:10])\n",
    "\n",
    "target_idxs = set(filter(lambda x: x != UNK_ix, [token2idx[w] for w in target_words]))\n",
    "\n",
    "print(\"Number of target words %d\" % len(target_idxs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calc_rewards(generated_batch):\n",
    "    assert generated_batch.ndim == 2\n",
    "    rewards = np.zeros_like(generated_batch)\n",
    "    is_alive = np.zeros_like(generated_batch, dtype=np.int32)\n",
    "    \n",
    "    for i in range(generated_batch.shape[0]):\n",
    "        for j in range(generated_batch.shape[1]):\n",
    "            if generated_batch[i][j] == EOS_ix:\n",
    "                break\n",
    "            if generated_batch[i][j] in target_idxs:\n",
    "                rewards[i][j] = 1\n",
    "            is_alive[i][j] = 1\n",
    "    return rewards, is_alive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('contexts.pkl', 'rb') as fin:\n",
    "    contexts = pickle.load(fin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'answer': 'Hey, Nick.',\n",
       " 'context': ['Kids can get pretty much anything they want in the yard, as long as they can afford it.',\n",
       "  \"'Cause everything comes with a price.\"]}"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "contexts[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def iterate_minibatches(batch_size=64):\n",
    "    total_batches = len(contexts)//batch_size\n",
    "    for b in range(total_batches):\n",
    "        excerpt = contexts[b*batch_size:(b+1)*batch_size]\n",
    "        excerpt = [item['context'] for item in excerpt]\n",
    "        yield phrase2matrix(excerpt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_phrases = [['Hello! How are you?'],\n",
    "                ['How old are you?'],\n",
    "                ['Are you fucking kidding me?'],\n",
    "                ['Suck. What are you doing?'], \n",
    "                ['You are piece of shit!!!'], \n",
    "                ['holy fucking crap. you are motherfucker']]\n",
    "\n",
    "test_phrases_ids = phrase2matrix(test_phrases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 0/100 epochs and 660/36806 batches in current epoch\n",
      "Loss (averaged with last 10 batches): 0.00164\n",
      "Avg reward: 0.00095\n",
      "Loss history:\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAD8CAYAAACb4nSYAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAIABJREFUeJzt3Xl8VPW9//HXJwkJ+xYCsgYEFMEFAQHrLmJBrXRRq9e2\n2tpSq9za9vYWrMuty63a25/eWq1LXep2xdZaRETRihU3kKCsIhj2nZAQIAGyfn5/zMkwmUySAQJJ\nZt7Px2MeOct3Zj6TTN7nO99z5hxzd0REJDmkNHYBIiJy9Cj0RUSSiEJfRCSJKPRFRJKIQl9EJIko\n9EVEkohCX0QkiSj0RUSSiEJfRCSJpDV2AdG6dOniffv2bewyRESalQULFuxw96z62jW50O/bty85\nOTmNXYaISLNiZuviaafhHRGRJKLQFxFJIgp9EZEkotAXEUkiCn0RkSSi0BcRSSIKfRGRJJJQob9k\n4y4Wbyxs7DJERJqsJvflrMPxtYc+AGDtvRc3ciUiIk1TQvX0RUSkbgp9EZEkotAXEUkiCn0RkSSi\n0BcRSSIKfRGRJBJX6JvZODNbYWa5ZjYlxvoMM3spWD/PzPpGrDvZzD42s2VmtsTMWjZc+SIicjDq\nDX0zSwUeBsYDg4GrzGxwVLPrgJ3uPgB4ALgvuG8a8DxwvbsPAc4FyhqsehEROSjx9PRHArnuvtrd\nS4GpwISoNhOAZ4Lpl4ExZmbAhcBid18E4O757l7RMKWLiMjBiif0ewIbIuY3BstitnH3cmAXkAkc\nB7iZzTKzT83sV7GewMwmmlmOmeXk5eUd7GsQEZE4HekduWnAmcDVwc9vmNmY6Ebu/ri7j3D3EVlZ\n9V7XV0REDlE8ob8J6B0x3ytYFrNNMI7fAcgn9KlgjrvvcPe9wExg2OEWLSIihyae0J8PDDSzfmaW\nDlwJTI9qMx24Jpi+DJjt7g7MAk4ys9bBxuAc4POGKV1ERA5WvWfZdPdyM5tEKMBTgafcfZmZ3Qnk\nuPt04EngOTPLBQoIbRhw951mdj+hDYcDM9399SP0WkREpB5xnVrZ3WcSGpqJXHZ7xPR+4PJa7vs8\nocM2RUSkkekbuSIiSUShLyKSRBT6IiJJRKEvIpJEFPoiIklEoS8ikkQU+iIiSUShLyKSRBT6IiJJ\nRKEvIpJEFPoiIklEoS8ikkQU+iIiSUShLyKSRBT6IiJJRKEvIpJEFPoiIklEoS8ikkQU+iIiSUSh\nLyKSRBT6IiJJRKEvIpJE4gp9MxtnZivMLNfMpsRYn2FmLwXr55lZ32B5XzPbZ2YLg9ujDVu+iIgc\njLT6GphZKvAwMBbYCMw3s+nu/nlEs+uAne4+wMyuBO4Dvh2sW+XuQxu4bhEROQTx9PRHArnuvtrd\nS4GpwISoNhOAZ4Lpl4ExZmYNV6aIiDSEeEK/J7AhYn5jsCxmG3cvB3YBmcG6fmb2mZm9Z2ZnHWa9\nIiJyGOod3jlMW4A+7p5vZsOBaWY2xN13RzYys4nARIA+ffoc4ZJERJJXPD39TUDviPlewbKYbcws\nDegA5Lt7ibvnA7j7AmAVcFz0E7j74+4+wt1HZGVlHfyrEBGRuMQT+vOBgWbWz8zSgSuB6VFtpgPX\nBNOXAbPd3c0sK9gRjJkdCwwEVjdM6SIicrDqHd5x93IzmwTMAlKBp9x9mZndCeS4+3TgSeA5M8sF\nCghtGADOBu40szKgErje3QuOxAsREZH6xTWm7+4zgZlRy26PmN4PXB7jfn8H/n6YNYqISAPRN3JF\nRJJIwoS+uzd2CSIiTV4ChX5jVyAi0vQlTug3dgEiIs1A4oS+uvoiIvVKmNCvVOaLiNQrYULfNcAj\nIlKvxAl9Zb6ISL0U+iIiSSRxQl/DOyIi9Uqc0Ffmi4jUK2FCv1KpLyJSr4QJfUW+iEj9Eif0lfoi\nIvVKoNBX6ouI1CeBQr+xKxARafoSJ/QbuwARkWYgYUJfR++IiNQvYUJfmS8iUr/ECX0N8IiI1Ctx\nQl+ZLyJSL4W+iEgSiSv0zWycma0ws1wzmxJjfYaZvRSsn2dmfaPW9zGzIjP7ZcOUXZOGd0RE6ldv\n6JtZKvAwMB4YDFxlZoOjml0H7HT3AcADwH1R6+8H3jj8cmunK2eJiNQvnp7+SCDX3Ve7eykwFZgQ\n1WYC8Eww/TIwxswMwMy+DqwBljVMybHpG7kiIvWLJ/R7Ahsi5jcGy2K2cfdyYBeQaWZtgcnAHYdf\nat2U+SIi9TvSO3J/Azzg7kV1NTKziWaWY2Y5eXl5h/REkaGvXr+ISGxpcbTZBPSOmO8VLIvVZqOZ\npQEdgHxgFHCZmf0O6AhUmtl+d38o8s7u/jjwOMCIESMOKbEjd+S6Q2hwSUREIsUT+vOBgWbWj1C4\nXwn8W1Sb6cA1wMfAZcBsD3W3z6pqYGa/AYqiA7+hVOvpH4knEBFJAPWGvruXm9kkYBaQCjzl7svM\n7E4gx92nA08Cz5lZLlBAaMNwVEWeeye0vVFXX0QkWjw9fdx9JjAzatntEdP7gcvreYzfHEJ9cfNa\npkVE5ICE/Eau9uOKiMSWQKEfMbyjvr6ISEyJE/qNXYCISDOQOKGv4R0RkXolTOjrylkiIvVLmNBX\nT19EpH6JE/poR66ISH0SJ/TV0xcRqVdihn7jlSEi0qQlTugTfRoGERGJljChrytniYjUL2FCv19m\nG0b16wxoeEdEpDYJE/odWrdg7OBugHbkiojUJmFCH8Cqrpyi0BcRiSmxQj/4qeP0RURiS6zQr+ro\nK/NFRGJKrNAPfirzRURiS6zQD7r6Ok5fRCS2BAv90E9FvohIbIkV+o1dgIhIE5dQoV9FozsiIrEl\nVuhXjelrgEdEJKaECv3w8I4yX0QkprhC38zGmdkKM8s1sykx1meY2UvB+nlm1jdYPtLMFga3RWb2\njYYtP7qO0E9lvohIbPWGvpmlAg8D44HBwFVmNjiq2XXATncfADwA3BcsXwqMcPehwDjgMTNLa6ji\na9RK1SGbR+oZRESat3h6+iOBXHdf7e6lwFRgQlSbCcAzwfTLwBgzM3ff6+7lwfKWHOFO+IGevlJf\nRCSWeEK/J7AhYn5jsCxmmyDkdwGZAGY2ysyWAUuA6yM2Ag0u/I1cZb6ISExHfEeuu89z9yHAacDN\nZtYyuo2ZTTSzHDPLycvLO+TnMh2oLyJSp3hCfxPQO2K+V7AsZptgzL4DkB/ZwN2XA0XAidFP4O6P\nu/sIdx+RlZUVf/W1UEdfRCS2eEJ/PjDQzPqZWTpwJTA9qs104Jpg+jJgtrt7cJ80ADPLBgYBaxuk\n8hgO7MhV7IuIxFLvkTTuXm5mk4BZQCrwlLsvM7M7gRx3nw48CTxnZrlAAaENA8CZwBQzKwMqgRvc\nfceReCFAeFBfmS8iEltch0+6+0xgZtSy2yOm9wOXx7jfc8Bzh1lj3DSkLyJSt8T6Rq7pOH0Rkbok\nVugHP3WcvohIbIkV+hrTFxGpU0KGvoiIxJZQoV9FHX0RkdgSKvR1nL6ISN0SK/R1amURkTolVOhX\nUUdfRCS2hAp9swMHbYqISE2JFfrBT/X0RURiS6zQ15i+iEidEiv0dfYdEZE6JVToV9HwjohIbAkV\n+rpGrohI3RIr9IOf6umLiMSWWKGvE66JiNQpoUK/qq+v4R0RkdgSKvTV0xcRqVtihX5jFyAi0sQl\nVujrcokiInVKqNAXEZG6JVTo6xq5IiJ1iyv0zWycma0ws1wzmxJjfYaZvRSsn2dmfYPlY81sgZkt\nCX6e37DlR9cR+qnhHRGR2OoNfTNLBR4GxgODgavMbHBUs+uAne4+AHgAuC9YvgP4mrufBFwDPNdQ\nhceuNfRTmS8iEls8Pf2RQK67r3b3UmAqMCGqzQTgmWD6ZWCMmZm7f+bum4Ply4BWZpbREIXHossl\niojULZ7Q7wlsiJjfGCyL2cbdy4FdQGZUm28Bn7p7yaGVGgf19EVE6pR2NJ7EzIYQGvK5sJb1E4GJ\nAH369Dnk50lLCaV+RaViX0Qklnh6+puA3hHzvYJlMduYWRrQAcgP5nsB/wC+5+6rYj2Buz/u7iPc\nfURWVtbBvYIIaSmhl1NWUXnIjyEiksjiCf35wEAz62dm6cCVwPSoNtMJ7agFuAyY7e5uZh2B14Ep\n7v5hQxVdmxapoZ5+eYV6+iIisdQb+sEY/SRgFrAc+Ku7LzOzO83s0qDZk0CmmeUCvwCqDuucBAwA\nbjezhcGta4O/ikBaaujllFeqpy8iEktcY/ruPhOYGbXs9ojp/cDlMe53N3D3YdYYt6ox/TL19EVE\nYkqob+S2qOrpK/RFRGJKqNBPqxrT1/COiEhMCRX6LcJH76inLyISS0KFfrinr0M2RURiSsjQL9OX\ns0REYkqo0K8a3lFPX0QktoQK/TR9OUtEpE4JFfpVh2yW6egdEZGYEir0q76cpZ6+iEhsCRX6qeFv\n5KqnLyISS0KFvpnRItV0nL6ISC0SKvQhdHplHb0jIhJb4oV+qlGu4/RFRGJKuNBvkZqiMX0RkVok\nXOinpZiO3hERqUXChX6L1BQdpy8iUouEC/20VPX0RURqk3ihn2IUFJdqXF9EJIaEC/3+WW35IHcH\nv39rRWOXIiLS5CRc6D/yneEcm9WGtTuKG7sUEZEmJ+FCPzXF6Noug4Li0sYuRUSkyUm40AfIbJNB\nvkJfRKSGhAz9zm3S1dMXEYkhrtA3s3FmtsLMcs1sSoz1GWb2UrB+npn1DZZnmtm7ZlZkZg81bOm1\n69wmncK9ZToHj4hIlHpD38xSgYeB8cBg4CozGxzV7Dpgp7sPAB4A7guW7wduA37ZYBXHoUvbdAB2\n7i07mk8rItLkxdPTHwnkuvtqdy8FpgITotpMAJ4Jpl8GxpiZuXuxu39AKPyPms5tMgDILy45mk8r\nItLkxRP6PYENEfMbg2Ux27h7ObALyIy3CDObaGY5ZpaTl5cX791q1blNqKdfUKRxfRGRSE1iR667\nP+7uI9x9RFZW1mE/XmYwvKMjeEREqosn9DcBvSPmewXLYrYxszSgA5DfEAUeisyqnn5xKaXl2pkr\nIlIlntCfDww0s35mlg5cCUyPajMduCaYvgyY7e6Ndtazjq3TSTH4aNUOjrv1DZ6bu66xShERaVLq\nDf1gjH4SMAtYDvzV3ZeZ2Z1mdmnQ7Ekg08xygV8A4cM6zWwtcD9wrZltjHHkT4NLTTG+OuQYZi3b\nBsCL89Yf6acUEWkW0uJp5O4zgZlRy26PmN4PXF7LffseRn2H7PxBXXlj6VYAikrKG6MEEZEmp0ns\nyD0S+nRuHZ4uVuiLiAAJHPrZmW3C03v2K/RFRCCBQ7/qW7kApTodg4gIkMChn5Za86VVVOoyiiKS\n3BI29KPd/9YKjr/1DfKLdGoGEUleSRP6D87OpbzS2br7qJ4GSESkSUma0K+yv0zj+yKSvJIw9Csa\nuwQRkUaTFKHfo0PL8PS+UoW+iCSvhA79fl1Cx+r/x4XHh5ftK6vgkX+tYuz97zVWWSIijSau0zA0\nV69OOoOdxaVkZ7ahV6dWfPvxuewvq+C+N79o7NJERBpFQvf027dsEf5mbv+ubYHqY/ol5Yk91LNl\n1z6ufPxjHaYqImEJHfqRWrZIBULDO1WKEvz0DE++v4a5qwv424KNjV2KSEKYtzqfW/6xpLHLOCzJ\nE/ppoZe6aMOu8LLos29u3bWfeasb7dovDa51Rmj07kifcO6h2V8yfdHmQ76/uzNr2VbKjtDpMopL\nymnEyztIAvn243N5Yd76Zn0Sx6QJ/arTMry+ZEt4WeHeMgDW7ihmZ3Ep331yXnjcvznYvnt/naeW\naJ0e+nRTXFJBflEJH686sEFbuW0PGwr2Nkgdv39rJT998bODus/8tQV876lPKC2v5J3l2/nxcwt4\n7L1VDVJPpH2lFZx8x1v84C/zG/yxj5ayikr6Tnmd5+O4GFBzvlLcqrwiFm8sbOwy6pQedB53NOMh\n06QJ/VgmPPwhMxZv5tzf/4tT73qbL7cXAbB0064abad9tonfzlx+UI9f1MA9zE2F+9i6K/SN4oLi\nUkb+9h1+/9aK8Pzjc1ZVe76S4ItoRSVlXP3EPK7681zKg970hQ/M4azfvXvYNR1qyPxs6kLmrMzj\n8y272bYn9JrWR2yE3v8yj8K9h36N49lfbGPRhkLyi0uoqHTeXZHH3tLm2TvbGVzr+c7XPq+xrqS8\ngsK9pazOK+Kv8zdw3K1vsD6/YTbmR9uY//celz70YXj+3RXbw+/3aEs27jqsw6937Stj6ifrD/r/\ns2rEIG9PfKH/xpItrNlRfND1HUkJffROPCb9X80e6hPvr2HKK0vo3Dqdv15/OpsK9/GzlxYCMHnc\nIFJTLNz2H59t5OyBWWS2zQgvu2fmcnp1asVtry7jtksGM39NATdfNIhu7VuSnprCLdOW0rJFCv/1\ntSHh+5RVVHLbtKWkphjd2rfkp2MGcs8by8lITeEHZ/ajY+t0zrh3NgBfH9qDLsHzTV+4mUtO7s7F\nD34AwFf6d+HEnh0A2LM/9EnmrzkHxvS/2LqHyx79qNrrrah0/m/eOi46qXu11xHN3SkoLg23KSmv\n4NuPza3R7j//tohVeUW8csMZNdbtL6vghYgrma3Yupuq/7sUs+D3v5q7X1/OGQMyefjfhtGxdXqN\nx4m2o6iEdfnFDM/uDMAP/pIDwBs3nRVu8+m6QsorKzl7YBbllc6e/WUxX++OohK2FO7npF4dYj7X\nym17KC2vpH9WWxas28mZA7swc8kWlm7axY/P6U+HVi1q3Gfb7v2UVVTSq1PrGI9Yu4pKJz8I/eiz\nxe4rreCE29+scZ/cvD30yTzwPBWVzvy1BYzq1xkzq9E+nhoefW8V3xzWk+4dWtVYvz5/L5c9+hF3\nThjCmh17+eFZ/WgR44SHB6Oy0vn+0/Pp2bEVH045v9q64pJyvvbQB5x3fBZPXXsaq/KKyWqbQUaL\nlPC+u1g2FOzl6ifm8aerh/HQ7FzeXLaVk3t1ZN6afC46qTvd2res9b5VWrZIZff+8nBPf9e+MtJS\njDYZNaO0tLySn7zwKa3TU/n8znEH+Rs4cpIq9P9w5VBumrqw3nZvLtsani6rqAyHLcDmwn0c06El\naSnGtU/P572VeZxzXBbP/GAkAHtLy3lszupw+7tmfB5+zPEnHsPPxx7Hi5+EQu+2iweTEmxAFqzb\nydT5G8L3u+7Mfjz2XuhxHpydy6rfXhReN23hgfHz8spKfv3KgR1Lj89ZTX5xCc9fNyrmFcOmfbap\n2qko3J0X5q3j9leXsX1PSbXvNET74+xc7n97JfNvuYCsdhm8v3IHCzcc+Djed8rrrLx7fHjH8fy1\nBVz/3AK+f0ZfenRsxVkDs/j7pxu5940Dh8y+unAzZw3MAsAs9E909+uhT1Qf5uYz9M63ueTk7jx4\n5ak4cP3zC7h8eC+O6dCS/WWVjOzXmf9+/XP+/P4aAJ68ZkT48aD6tRQem7OK97/cwfXn9GfZ5l28\n/+UOlvzmQtq1bMHmwn08Pmc1k8cNYsTd/wRgzT0XxQzJCx+YA8C/jerD/81bT0ZaCiXBJ54//WsV\nV4/qw4ShPRnZL7QBWrihkK8/HOrBvnHTWZzQvT0Q2oc07g9zePDKUxme3YnW6ak8/G4uF5/cI/wd\nk+F3v02b9AP/pr+Zvoy3P99GyxYpTBl/Qsy/046iUopLylm8cRfDszvxwrx13PHa5zxy9TDGn9Q9\n5n3cnQfeXsmlQ3syIDjSrcr7X+bxP7NW8NayrWSkpdK/axvu+ebJ4fWfrC1g+54Srn/+UwBapBo/\nPOtYAG6a+hmtWqRy77dOrvaY7k7h3jI6tUnnodlfcu7xXcPr9pdV8MGXO4DQp9vpizaTX1TCmEHd\nmL5oU3jD8+6KPD7I3cF3n/wkfN/XJp3JkB7tefqjtbz4yXp6dmzFz8cexym9OvDIe6tYX7CXGYu3\nsHpH6FP9Ux+u4eUFG5m2cDOv3ngGa3YUs7+sglcXbmbyuOOpdKp18qo2KtuDnv7QO9+iR4fqG6b5\nawsY3qcT6/JDPfy9pRU889FaLj65e7izFsvWXfvJbJt+2BvM+iRV6E8Y2pOMtFR+9+YXYHB8t3as\nzitmxbY94TbD+nTk0/UHgixyHBxg9Y5i/vPlRcxbUxDuoW4LTuKWt6eEe+oYAnpj6VY+ini8TYX7\n6N25NUUl5Wzaua9a20/X76w2/68V22M+5rbdJWzbfeCjZtUO1cK9ZeyJEfpPfLCm2vzU+Ru4/dVl\nQM2Lzdz/1gr+/P4alt3xVVJSLDzmvr5gL1ntMlgcYxhsW8QJ7S5/9GMgNOYPcFrfTpw54EAgn9qn\nY7XfR+HeMk65460ajzlj8RZGHZvJ4O7tefvzbbz9+bbwui5tM6qNr173TA53XHrgE1Tk4aqrguG7\nRyP2Hfxz+TbOGpjFbdOW8s4X26vtz9m8az9zVubxrWG9SE9LobLSw+EO8FKwkS6JGuJ6Yd56Xpi3\nnjduOosBXdvy0aod4XWPvbeKL7bu4bV/P5MPc3dQuLeM7z31CempKVxxWi+en7ue1xZtYdbPz2ZT\n4T4K95aF9z0B/OWjteHpic/l1PhdAcxZmcevXl4MwI3n9efhd0Ov9ycvfMp/jD2OG84bwNT567nk\n5B7hTyUbd+7jwdm5PDg7l6kTR7Nk4y7e+WIbf7xqGK8vDu0H27BzHwXFpXyytoDKShg7uBsXDO7G\nss3V3wcvzFvPxp376NwmnVeDDkp06M9dXcBVf57LlPGD+P1bK8PvEYD84lJ++OyB11a1v+iOGMNb\nn2/eXW3+aw99wF++f1q4s5W7vYj3VubRrX1G+P9k7Y7i8FDiy0EHZdGGQmYs3lztk3/u9iLeXbGd\np649ja279tG9Q6vw/T5bX8gHX+bgHvo/Liopp016avh1dW2XwS0XH9go/9f0Zcxdnc8j3xkOhD7J\n3PvmF/Ts2IprvtIXgO89NY/szDb8+XsjarzOhmRN7aiGESNGeE5O7DfzkXD1E3P5MDefWy8+gfat\nWnD+oK7hnl6k//7Gidzyj6VMOm8AD72bW23d8OxOPPRvp3L6PbNr3K9KWopRfhDn88/ObM26iLHZ\n847P4t0VebW2/+cvzuaC++eE50/r24n5a3fW2j6WYX06cnKvjvzgjH6s3lHEtU+Hdn5eNrwX/bq0\n4X9mrQi3XXDrBfzyb4vYXLifsYO7hX8nj35nWLjHF61zm3S+MzqbB9/5EoBPbhnDWfe9WyM0YxmR\n3YmcdfG9ngtO6MY/l2+rvyHQs2MrNhXuq3PdNadnc+N5Axj523fiesx4/Pl7I/jRs7Hf513apvPK\nT87g1YWb+H9vr4zZJl5m4A7jTzyGj1blYwa/Hn8Cv/r7Ytq1TOPik7rTskUqx3Vrx69jHIo4PLsT\nK7buqfU605ec3J0Ziw8cHNE3szVrY+xTuO2SweRuL+KtZVv55rCedGvfMvyJLtqvxh3P795cEXNd\nU/b1oT04sWeH8Ovq2i4j/ImgSv+sNmzfU1Ktg3XN6dmcO6gr3396PpPOG8Avv1r7p+26mNkCd693\ni5H0of+zqZ8xbeFmnrxmBGNO6AaEdoqedd9siiN2FE278YzwR/RYjuvWlpXbimKue2niaE7p3ZFB\nt9Ucfz1Yj1w9jMl/X8zuqF75F3eN447XlvHiJxtquWfd0tNSwjtl22WkxfyUEOnar/TltUWbOff4\nrpgd6DFFW3vvxfSd8joAKQZV2727vn4i3x2dzU1TPwv3Bqv88xfncEEdp8n49ojevJRT++tsk55a\n7W8XuaxDqxYsvH0st0xbyvr8vXyQe6AXnppiR+xCO0N7d2Tltj3sjbHz8VfjjqddRhq3BZ+4oj+9\nxHLFiF5UOry5dGutgRxp1W8vYvmW3Vz60AdEvsTov3VkZyFyg3hC9/Ys31K9Vx3Liz8aTbuWaVzy\nxw/qbPf9M/ry9Idr6328eMVb39HWqXULdkZ8UqvPg1edyqWn9Dik54o39JP66B2A2782hOvP6c/Z\nxx0YdujcJp3Zvzy3Wruu7TL46fkDqi1rG7HzJjLw04IxwOO6teW7o7MZdWwmLVukMuc/z6Nnx1bc\nevEJ9OpUc4dYlbu+fmKt6y4ccgwv/+QrNZa3bJHKPd88mS/uqr7D6LffOImnrz0tPI5cZfK4QeHp\ni0/uzuv/fiZjBoXGVatC4IoRvard59rgYyiEhhnyi0s5qWd7bji3P+dE/P4gtHF6bdKZQGiMF6gW\nNt8dnQ3AV/pnAnDmgC786Kx+3HHpEAZ0bcvN4wdV+/1We03fPIlbL64+nn1M+5Y8/f3T+OqQbhSX\nVpCRlsKnt40Nr6/6Ut6ufWWYGb/9xkk8/8NRDDqmXbjN/VecEp7+09XD+NopPZgwtOY/4GXDD/xe\nvj60BwODMfDTj80ML//ZBQOr/R0z0lI4pkPsHYU3nDuA74zODj9XbYHfItXo1Do0HDOwazt+f/kp\nzPnVefz4nGMjHqs/a+65iLX3XsyT14xgYNe2fH1oD1JTjBN7duD5H47i0lN68NLE0bz/q/OY8dMz\nw/c9c0AXTu/fJTx//bn9w9NjB3eLWVOVnh1b8dLE0ZzePzN8IEEsI/uG9nNEBn5qinH1qD7Mv+WC\nOp8j2rQbDxwoUFEZ31FkmW0OHBQw8exjObZLG/5w5VBO7dORthlpzPj3M8M1Rrt5/CAuG94rvP8O\nYMXdde+gPW9QV+6u4/852uhjYz93Q4prTN/MxgF/AFKBJ9z93qj1GcCzwHAgH/i2u68N1t0MXAdU\nAD9191kNVn0D6NwmnSnjB9VYHr0nP6tdBr+48Hh+PvY4NhTsY/f+Mrq1b8nrizfzm9c+54Tu7blp\nzABO6d2R7h1aUVpeSYtUq7YjsE9m6/AOnytH9qHSnZ3FpfTpHDrS4tZpS/loVT7fHZ3NbdOW0jez\nNVPGh476+cafQkfcpKYYfYNTS1w1sje9OrUmv+jAoY0tW6Ty6HeGc/3zC+jWPoOrRvbGzDhvUFfy\n9pTw3Nx1XDi4G0N6tA+fg+jYLm0Y2K0dT157Gp+sKeCKxz5mVL/O/O6yU8JH/ky78QyG9u7I+YO6\nkl9cwt+Pv6mIAAAKnUlEQVQXbKJflzZ87ZQeZLbN4KlrT6P/r2eG6/jqkGPCO6mnTzqTe9/4gvdW\nhoanfn3Rgd/34O6hgBjQtS23XDw4vPzH5/Tnx+f0D39K+Pjm8zn9ntmM7NuZ1BSjR8cDG82eHVsx\n6+dn0zYjjaUbdzFr2TaGZ3eic5t0TurZgSWbdjGsT2h4qH9Wm2p/15+OGcgNL3zKsz8YybDsTuG/\n9UUndeeik7pTUel8/4x+5KwtCH9sj3zubw3vxf1XdOF/3/mSK0b04kfPLmD5lt2MPjaTUf068+W2\nPTz78TouObl7tR310cyMP1x5KpPOG8DYB+ZUW/fsD0YysFtburVrSVFpOf/z5gquGtUHCL1/B0ds\n0Ht3bh1+z405oVv402uVr/Tvwlcigr0yYkv8/A9H8cqnob93qxapXDGiFyf2aM+STbtiHpF037dO\n4pgOrRh9bGdSzKrtgPz38wfwx9mhIb+nrz2N15ds4bzju9I6I5VPni4A4Nzjs+jQqgV3XDqkziO0\nXrnhK3zzTx/xyNXDWFewl+KScrq2y2Bo74789PwBVLjTqXV6teGiCUN7cP05/fnBX+azJeKwz67t\nWzL/lgv4bEMhw7M78euLQp2HcSceQ2UltEpP5a/Xn86wu96moLj6IcPpaSn8/vJTqi3LSEvl7z85\nna7tWnL/2yv5x2eb6Ni6BWkpKewoKuGknh34zuhsbp22FICvDunGrGXb+MXY4zhjQBe+9ciBI+l+\nMfY4urar/wiiw+budd4IBf0q4FggHVgEDI5qcwPwaDB9JfBSMD04aJ8B9AseJ7Wu5xs+fLg3Fbnb\n9/i23fsa5bkL95b6vtLy8PyzH63xP76zMjxfUFTiZeUVMe+7YF2BZ0+e4bf8Y3Gdz3He/7zr2ZNn\n+OINhdWWL1y/07cUhl539uQZnj15hldUVNZb8/j/nePZk2f44++tqrGuoqLSn/t4ra/curvGutnL\nt1V7rZFeX7zZX87Z4O7ua3cU+Z79Ze7uvmRjoWdPnuFX/3lutfZLN4WW/2vFdnd3Ly4p89V5RV5Q\nVOLLt+zyncUlNZ5j2659XlkZen2vfLrBV23fE7OW9fnFfvWf5/rC9TvDr3XHnv01Huuu15b5/rLy\nassqKyv98kc/Cv8+syfP8CG3v+nr84ur3b+ysjK8/on3V/uMRZtj1hJp+ZZdnj15hk98dn5cf6do\nH+XuCNdRXlHpL85bV61+d/e/5Wzw7Mkz/EfPzPcX563zJRsLYz1UNcPufMuzJ8/wouBv5u6+aede\nz548w699al7M+/zk+Rz/zhNzffe+Un9z6Rb/clvo/VL196lNZWWlFxaXevbkGX7Xa8uqrZuzcru/\nsWSzZ0+e4V994L1663YP/Q88+M+V4b/Fz6Z+5rv3lYbXv7Zokz/yr9xq99lfVu7XP5fjSzcV+s+m\nfubZk2f4h7l57u5++SOhv315RaXnrM0P32fW0i2es7bAB9/2Rvi1Hiogx+vJc3ePK/RPB2ZFzN8M\n3BzVZhZwejCdBuwALLptZLvabk0p9Juz1xdv9r0lsYO0St6e/f6PTzfW+Q/1f/PW+c2v1L3xqFK0\nv8y/3BY7MI+EtTuKvLikrMbykrLYG8OGtKVw30F3CHK37/Ebnl8QDpIX5q6L2e6KRz/yE//rzYN6\n7PyimhuzhrQ+v9izJ8/wT9bk1984sHZHkS/fsqvG8p3FJfWG+KEq3Fvq5TE2fF9s2e3Zk2f4Nx7+\n4KAeb9GGnf5e0IE4GHv2l/lzH68Nb4RLyipivlcbUryhX++OXDO7DBjn7j8M5r8LjHL3SRFtlgZt\nNgbzq4BRwG+Aue7+fLD8SeANd3+5tuc72jtyRY62+WsL6J/Vls5tYg9plFdUUukHvvIvh8/deeCf\noSG4g/2CXHMR747cJnGcvplNBCYC9OnTp5GrETmyTqtlR2GVtCP85ZxkZGb8YuxxjV1GkxDPu2sT\n0DtivlewLGYbM0sDOhDaoRvPfXH3x919hLuPyMrKil4tIiINJJ7Qnw8MNLN+ZpZOaEft9Kg204Fr\ngunLgNnBGNN04EozyzCzfsBA4BNERKRR1Du84+7lZjaJ0E7YVOApd19mZncS2nEwHXgSeM7McoEC\nQhsGgnZ/BT4HyoEb3b15nLdYRCQBJf03ckVEEoG+kSsiIjUo9EVEkohCX0QkiSj0RUSSSJPbkWtm\neUD9V4CuXRdCp4FoblT30dVc64bmW7vqPrKy3b3eLzo1udA/XGaWE88e7KZGdR9dzbVuaL61q+6m\nQcM7IiJJRKEvIpJEEjH0H2/sAg6R6j66mmvd0HxrV91NQMKN6YuISO0SsacvIiK1SJjQN7NxZrbC\nzHLNbEpj1xPJzJ4ys+3BxWaqlnU2s7fN7MvgZ6dguZnZg8HrWGxmwxqx7t5m9q6ZfW5my8zspmZU\ne0sz+8TMFgW13xEs72dm84IaXwrOHEtwJtiXguXzzKxvY9Ue1JNqZp+Z2YzmUreZrTWzJWa20Mxy\ngmXN4b3S0cxeNrMvzGy5mZ3eHOo+VAkR+maWCjwMjCd0Xd6rzGxw3fc6qv4CjItaNgV4x90HAu8E\n8xB6DQOD20TgkaNUYyzlwH+4+2BgNHBj8HttDrWXAOe7+ynAUGCcmY0G7gMecPcBwE7guqD9dcDO\nYPkDQbvGdBOwPGK+udR9nrsPjTjEsTm8V/4AvOnug4BTCP3em0Pdhyaeayo29RtxXMe3sW9AX2Bp\nxPwKoHsw3R1YEUw/BlwVq11j34BXgbHNrXagNfApoUt47gDSot831HKd50aqtxehoDkfmEHoetPN\noe61QJeoZU36vULogk9ron9nTb3uw7klRE8f6AlsiJjfGCxryrq5+5ZgeivQLZhukq8lGDY4FZhH\nM6k9GCJZCGwH3gZWAYXuXh6jvnDtwfpdQObRrTjsf4FfAZXBfCbNo24H3jKzBRa6BCo0/fdKPyAP\neDoYTnvCzNrQ9Os+ZIkS+s2ah7oMTfYwKjNrC/wd+Jm7745c15Rrd/cKdx9KqOc8EhjUyCXVy8wu\nAba7+4LGruUQnOnuwwgNgdxoZmdHrmyi75U0YBjwiLufChRzYCgHaLJ1H7JECf24rsXbxGwzs+4A\nwc/twfIm9VrMrAWhwH/B3V8JFjeL2qu4eyHwLqFhkY4Wuo4zVK+vtus8H21nAJea2VpgKqEhnj/Q\n9OvG3TcFP7cD/yC0oW3q75WNwEZ3nxfMv0xoI9DU6z5kiRL68VzHt6mJvK7wNYTGy6uWfy84SmA0\nsCviY+ZRZWZG6FKYy939/ohVzaH2LDPrGEy3IrQvYjmh8L8saBZde6zrPB9V7n6zu/dy976E3sez\n3f1qmnjdZtbGzNpVTQMXAktp4u8Vd98KbDCz44NFYwhd3rVJ131YGnunQkPdgIuAlYTGbW9p7Hqi\nansR2AKUEepZXEdo3PUd4Evgn0DnoK0ROhJpFbAEGNGIdZ9J6GPtYmBhcLuomdR+MvBZUPtS4PZg\n+bHAJ0Au8DcgI1jeMpjPDdYf2wTeN+cCM5pD3UF9i4Lbsqr/wWbyXhkK5ATvlWlAp+ZQ96He9I1c\nEZEkkijDOyIiEgeFvohIElHoi4gkEYW+iEgSUeiLiCQRhb6ISBJR6IuIJBGFvohIEvn/KYHWlKVg\nKicAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f6d518b8b38>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answers on test phrases:\n",
      "Phrase:\t ['Hello! How are you?']\n",
      "Answer:\t\n",
      "\"i\"(0.013) \n",
      "\"'\"(0.010) \n",
      "\"m\"(0.010) \n",
      "\"not\"(0.008) \n",
      "\"welcome\"(0.012) \n",
      "\".\"(0.005) \n",
      "---------------\n",
      "Phrase:\t ['How old are you?']\n",
      "Answer:\t\n",
      "\"i\"(0.015) \n",
      "\"'\"(0.014) \n",
      "\"m\"(0.010) \n",
      "\"11\"(0.010) \n",
      "\".\"(0.005) \n",
      "---------------\n",
      "Phrase:\t ['Are you fucking kidding me?']\n",
      "Answer:\t\n",
      "\"all\"(0.048) \n",
      "\"right\"(0.012) \n",
      "\",\"(0.006) \n",
      "\"you\"(0.029) \n",
      "\"ready\"(0.065) \n",
      "\"for\"(0.046) \n",
      "\"the\"(0.007) \n",
      "\"rod\"(0.017) \n",
      "\"!\"(0.005) \n",
      "---------------\n",
      "Phrase:\t ['Suck. What are you doing?']\n",
      "Answer:\t\n",
      "\"it\"(0.040) \n",
      "\"'\"(0.010) \n",
      "\"s\"(0.010) \n",
      "\"something\"(0.012) \n",
      "\"you\"(0.005) \n",
      "\"like\"(0.008) \n",
      "\"?\"(0.007) \n",
      "---------------\n",
      "Phrase:\t ['You are piece of shit!!!']\n",
      "Answer:\t\n",
      "\"have\"(0.045) \n",
      "\"a\"(0.014) \n",
      "\"sharper\"(0.011) \n",
      "\"intervention\"(0.007) \n",
      "\",\"(0.005) \n",
      "\"doc\"(0.015) \n",
      "\".\"(0.006) \n",
      "---------------\n",
      "Phrase:\t ['holy fucking crap. you are motherfucker']\n",
      "Answer:\t\n",
      "\"really\"(0.050) \n",
      "\"?\"(0.018) \n",
      "---------------\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-84-a3392196c30e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0;31m## Training stuff.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m         \u001b[0mwords_seq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdec_cell_seq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGenTest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0mrewards\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_alive\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcalc_rewards\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwords_seq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.5/site-packages/theano/compile/function_module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 873\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0moutput_subset\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    874\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_subset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_subset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.5/site-packages/theano/scan_module/scan_op.py\u001b[0m in \u001b[0;36mrval\u001b[0;34m(p, i, o, n, allow_gc)\u001b[0m\n\u001b[1;32m    955\u001b[0m         def rval(p=p, i=node_input_storage, o=node_output_storage, n=node,\n\u001b[1;32m    956\u001b[0m                  allow_gc=allow_gc):\n\u001b[0;32m--> 957\u001b[0;31m             \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    958\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    959\u001b[0m                 \u001b[0mcompute_map\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.5/site-packages/theano/scan_module/scan_op.py\u001b[0m in \u001b[0;36mp\u001b[0;34m(node, args, outs)\u001b[0m\n\u001b[1;32m    944\u001b[0m                                                 \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    945\u001b[0m                                                 \u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 946\u001b[0;31m                                                 self, node)\n\u001b[0m\u001b[1;32m    947\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mImportError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtheano\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgof\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMissingGXX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    948\u001b[0m             \u001b[0mp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mtheano/scan_module/scan_perform.pyx\u001b[0m in \u001b[0;36mtheano.scan_module.scan_perform.perform (/root/.theano/compiledir_Linux-4.2--generic-x86_64-with-debian-jessie-sid-x86_64-3.5.2-64/scan_perform/mod.cpp:4490)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.5/site-packages/theano/gof/op.py\u001b[0m in \u001b[0;36mrval\u001b[0;34m(p, i, o, n)\u001b[0m\n\u001b[1;32m    858\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mparams\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNoParams\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    859\u001b[0m             \u001b[0;31m# default arguments are stored in the closure of `rval`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 860\u001b[0;31m             \u001b[0;32mdef\u001b[0m \u001b[0mrval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnode_input_storage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnode_output_storage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    861\u001b[0m                 \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    862\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 512\n",
    "N_EPOCHS = 100\n",
    "VERBOSITY = 10 # number of batches before printing\n",
    "NUM_BATCHES = len(contexts)//BATCH_SIZE\n",
    "\n",
    "\n",
    "loss_history = []\n",
    "rewards_history = []\n",
    "for n_epoch in range(N_EPOCHS):\n",
    "    for nb,batch in enumerate(iterate_minibatches(BATCH_SIZE)):\n",
    "        ## Printing stuff.\n",
    "        if (n_epoch*NUM_BATCHES + nb + 1) % VERBOSITY == 0:\n",
    "            clear_output(wait=True)\n",
    "            print(\"Processed {}/{} epochs and {}/{} batches in current epoch\".format(n_epoch, N_EPOCHS,\n",
    "                                                                                     nb+1, NUM_BATCHES))\n",
    "            print(\"Loss (averaged with last 10 batches): {0:.5f}\".format(np.mean(loss_history[-10:])))\n",
    "            print(\"Avg reward: {0:.5f}\".format(np.mean(rewards_history[-10:])))\n",
    "            print(\"Loss history:\")\n",
    "            plt.plot(loss_history)\n",
    "            plt.show()\n",
    "            \n",
    "            print(\"Answers on test phrases:\")\n",
    "            words_seq, dec_cell_seq = GenTest.generate(test_phrases_ids)\n",
    "            critic_values = Critic.predict(dec_cell_seq)\n",
    "            \n",
    "            answers = []\n",
    "            for i in range(words_seq.shape[0]):\n",
    "                answer = words_seq[i]\n",
    "                if EOS_ix in answer:\n",
    "                    answer = answer[:list(answer).index(EOS_ix)]\n",
    "                answers.append([tokens[idx] for idx in answer])\n",
    "            \n",
    "            for i in range(len(test_phrases)):\n",
    "                print(\"Phrase:\\t\", test_phrases[i])\n",
    "                answer_tok_critic_value = zip(answers[i], critic_values[i])\n",
    "                print(\"Answer:\\t\",)\n",
    "                for tok,value in answer_tok_critic_value:\n",
    "                    print('\"{}\"({:.3f}) '.format(tok, value),)\n",
    "                print('---'*5)\n",
    "            \n",
    "        \n",
    "        ## Training stuff.\n",
    "        words_seq, dec_cell_seq = GenTest.generate(batch)\n",
    "        \n",
    "        rewards, is_alive = calc_rewards(words_seq)\n",
    "        \n",
    "        batch_loss = CriticTrainer.train_step(rewards, is_alive, dec_cell_seq)\n",
    "        \n",
    "        loss_history.append(batch_loss)\n",
    "        rewards_history.append(np.mean(rewards))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some answer generations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i ' m not a fan of the play .\n",
      "i don ' t know .\n",
      "the video .\n",
      "uh , uh , i ' m just gonna go with the . . .\n",
      "that is . . .\n"
     ]
    }
   ],
   "source": [
    "TEMPERATURE.set_value(np.float32(0.5))\n",
    "for i in range(5):\n",
    "    print(GenTest.reply([\"what is your favorite show?\"], 20))\n",
    "    \n",
    "TEMPERATURE.set_value(np.float32(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hedgedir/agentnet/agentnet/agent/recurrence.py:188: UserWarning: State_variables recommended type is OrderedDict.\n",
      "                Otherwise, order of agent state outputs from get_sessions and get_agent_reaction methods\n",
      "                may depend on python configuration.\n",
      "\n",
      "                Current order is: [<lasagne.layers.merge.ElemwiseMergeLayer object at 0x7f0176a7cc88>, <lasagne.layers.special.NonlinearityLayer object at 0x7f0176a7cc50>]\n",
      "                You may find OrderedDict in standard collections module: from collections import OrderedDict\n",
      "                \n",
      "  \"\"\".format(state_variables=list(self.state_variables.keys())))\n",
      "/home/hedgedir/agentnet/agentnet/agent/recurrence.py:301: UserWarning: You are giving Recurrence an input sequence of undefined length (None).\n",
      "Make sure it is always above <unspecified>(n_steps) you specified for recurrence\n",
      "  \"Make sure it is always above {}(n_steps) you specified for recurrence\".format(n_steps or \"<unspecified>\"))\n",
      "INFO (theano.gof.compilelock): Refreshing lock /root/.theano/compiledir_Linux-4.2--generic-x86_64-with-debian-jessie-sid-x86_64-3.5.2-64/lock_dir/lock\n",
      "INFO (theano.gof.compilelock): Refreshing lock /root/.theano/compiledir_Linux-4.2--generic-x86_64-with-debian-jessie-sid-x86_64-3.5.2-64/lock_dir/lock\n",
      "INFO (theano.gof.compilelock): Refreshing lock /root/.theano/compiledir_Linux-4.2--generic-x86_64-with-debian-jessie-sid-x86_64-3.5.2-64/lock_dir/lock\n",
      "INFO (theano.gof.compilelock): Refreshing lock /root/.theano/compiledir_Linux-4.2--generic-x86_64-with-debian-jessie-sid-x86_64-3.5.2-64/lock_dir/lock\n"
     ]
    }
   ],
   "source": [
    "import lasagne \n",
    "class trainer:\n",
    "    \"\"\"contains a recurrent loop where network is fed with reference answers instead of her own outputs.\n",
    "    Also contains some functions that train network in that mode.\"\"\"\n",
    "    #training recurrence\n",
    "    reference_answers = T.imatrix(\"decoder reference answers\")\n",
    "    \n",
    "    bos_column = T.zeros((reference_answers.shape[0], 1), 'int32')+BOS_ix\n",
    "    reference_answers_bos = T.concatenate((bos_column, reference_answers ), axis=1)  #prepend BOS\n",
    "        \n",
    "    l_ref = InputLayer((None, None), reference_answers_bos, name='context input')\n",
    "    l_ref_mask = InputLayer((None, None), T.neq(reference_answers_bos, PAD_ix),'context mask')\n",
    "\n",
    "    recurrence = Recurrence(input_sequences={decoder.inp_word: l_ref},\n",
    "                            input_nonsequences={decoder.encoder_lstm: encoder.output},\n",
    "                            state_variables={decoder.new_cell: decoder.prev_cell,\n",
    "                                             decoder.new_hid: decoder.prev_hid},\n",
    "                            tracked_outputs=[decoder.next_word_probs],#<ADDWHATEVERYOUWANT>,]\n",
    "                            mask_input=l_ref_mask,\n",
    "                            unroll_scan=False)\n",
    "    \n",
    "    \n",
    "    P_seq = get_output(recurrence[decoder.next_word_probs])\n",
    "    #V_seq = whateveryouadded\n",
    "    \n",
    "    ############################\n",
    "    ###loglikelihood training###\n",
    "    ############################\n",
    "    predicted_probas = P_seq[:, :-1].reshape((-1, N_TOKENS))+1e-6\n",
    "    target_values = reference_answers.ravel()\n",
    "    llh = lasagne.objectives.categorical_crossentropy(predicted_probas, target_values)\n",
    "    llh_loss = llh.mean()\n",
    "\n",
    "    #only train over generator weights since state value estimator does not influence likelihood (disconnected)\n",
    "    llh_updates = lasagne.updates.adam(llh_loss, generator.weights, 0.001)\n",
    "\n",
    "    train_llh_step = theano.function([encoder.input_phrase, reference_answers], llh_loss, updates=llh_updates)\n",
    "    get_llh = theano.function([encoder.input_phrase, reference_answers], llh)\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
