{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python2.7/site-packages/matplotlib/font_manager.py:273: UserWarning: Matplotlib is building the font cache using fc-list. This may take a moment.\n",
      "  warnings.warn('Matplotlib is building the font cache using fc-list. This may take a moment.')\n"
     ]
    }
   ],
   "source": [
    "#%env THEANO_FLAGS=device=gpu\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from utils.southpark import get_conversations\n",
    "conversations = get_conversations(\"../../sp.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tqdm\n",
      "  Downloading tqdm-4.9.0-py2.py3-none-any.whl (42kB)\n",
      "\u001b[K    100% |████████████████████████████████| 51kB 828kB/s \n",
      "\u001b[?25hInstalling collected packages: tqdm\n",
      "Successfully installed tqdm-4.9.0\n",
      "\u001b[33mYou are using pip version 8.1.2, however version 9.0.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install tqdm\n",
    "!pip install regex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7285/7285 [00:09<00:00, 801.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22247 out of 22247 tokens, coverage=1.00000)\n"
     ]
    }
   ],
   "source": [
    "from utils.preprocessor import Preprocessor\n",
    "#preprocessor = tokenizer + token_to_ix\n",
    "\n",
    "#one can automatically create one from corpora\n",
    "preproc = Preprocessor.from_conversations(conversations,max_tokens=30000,verbose=True)\n",
    "\n",
    "\n",
    "#or create manually from tokens + tokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7285\n",
      "Garrison:\n",
      "\tson of a bitch ! \n",
      "Randy:\n",
      "\twhat ?  to take one area of town that was rappy and gentrify it for the local people to enjoy ?  i thought we could keep it contained . \n",
      "Victoria:\n",
      "\tit doesn ' t contain .  what ' s happened to south park is happening everywhere .   thirty miles south of here in the town of fairplay ,  they ' ve changed the area north od downtown into nodofopa .  a rundown area south of the capital in cheyenne ,  wyoming ,  is now historic socacheywo .  channel street in mid - chicago is being revitalized into chimichanga . \n",
      "Randy:\n",
      "\toh my god . \n",
      "Victoria:\n",
      "\tlodo ,  sobro ,  rivmo ,  all happening at the same time .  and it isn ' t just in the u . s .  in cairo ,  the area northwest of the third pyramid is nowe3pi .  three miles north of auschwitz is nomoauchie .  it goes on and on ! \n",
      "Randy:\n",
      "\twhat does it mean ? \n",
      "Garrison:\n",
      "\tin our town it all started when pc principal arrived .  he ' s part of a mjuch larger conspiracy ,  and you ' re his lackey . \n",
      "Randy:\n",
      "\tnot me .  if pc principal has been using us ,  i ' ll take the bastard down myself . \n",
      "Reporter:\n",
      "\tit ' s day two of the hunger strike started by the college - aged fraternity brothers who are demanding that all of south park ' s community leaders step down .  the pc frat brothers say they ' ve gone now two days without eating any pussy ,  and will continue to do so until people resign . \n",
      "Tom:\n",
      "\twho is that reporter ?  do we .  .  .  know him ? \n",
      "Kevin:\n",
      "\ttom ,  that ' s bill keegan ,  wcfo . \n",
      "Tom:\n",
      "\tthanks ,  brian .   he ' s working for the ads ,  obviously .  sellout douche - bag . \n",
      "Newsman:\n",
      "\tthat ' s right ,  tom .  he was always a douche - bag at the conventions . \n",
      "Tom:\n",
      "\tthanks ,  rick .  stay dry . \n"
     ]
    }
   ],
   "source": [
    "print len(conversations)\n",
    "\n",
    "for speaker,phrase in conversations[42]:\n",
    "    print speaker,'\\t',preproc.preprocess_phrase(phrase)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# model\n",
    "See models/twoline_model.py and others for more incomprehensible bullshit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#max phrase length\n",
    "max_len=15\n",
    "\n",
    "#dictionary size\n",
    "n_tokens = len(preproc.tokens)\n",
    "\n",
    "#n lines in context NOT USED IN THIS EXAMPLE\n",
    "context_size=3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word_embedding_size = 128\n",
    "gru0_units=512\n",
    "grad_clip=5\n",
    "\n",
    "#IMPORTANT\n",
    "unroll_scan = False \n",
    "#if true, compiles longer, but runs ~20% faster on most GPUs (by unrolling & optimizing recurrent loop)\n",
    "# as a draw back, if true, you will have to recompile "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import theano\n",
    "import theano.tensor as T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#input phrase goes here\n",
    "prev_phrase = T.imatrix('user input line[batch,token_ix]')\n",
    "\n",
    "batch_size = prev_phrase.shape[0]\n",
    "#mask-out the PAD tokens for short phrases\n",
    "prev_phrase_mask = T.neq(prev_phrase,preproc.token_to_ix[\"PAD\"],)\n",
    "\n",
    "#reference answer goes here\n",
    "reference_answer = T.imatrix('reference answer[batch,token_ix]')\n",
    "reference_mask = T.neq(reference_answer,preproc.token_to_ix[\"PAD\"],)\n",
    "\n",
    "#sampling temperature\n",
    "greed = theano.shared(np.float32(1.),name='decoder greed')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import lasagne\n",
    "from lasagne.layers import InputLayer,DenseLayer,GRULayer,EmbeddingLayer\n",
    "from hsoftmaxplayer import HierarchicalSoftmaxDenseLayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "###ENCODER\n",
    "\n",
    "#class used as a dictionary, not actual class\n",
    "class encoder:\n",
    "    l_in = InputLayer((None,None),prev_phrase,name='prev phrase input')\n",
    "    l_emb = EmbeddingLayer(l_in,n_tokens,word_embedding_size, name=\"prev phrase embedding\")\n",
    "    l_mask = InputLayer((None,None),prev_phrase_mask,'prev phrase mask')\n",
    "    l_gru0 = GRULayer(l_emb,\n",
    "                      gru0_units,\n",
    "                      name='gru0',\n",
    "                      grad_clipping=grad_clip,\n",
    "                      mask_input = l_mask,\n",
    "                      only_return_final=True)\n",
    "    output=l_gru0\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from agentnet.memory import GRUCell\n",
    "from agentnet.resolver import ProbabilisticResolver\n",
    "from agentnet.agent import Recurrence\n",
    "\n",
    "###one-step update of decoder recurrence. Used for both training and generation.\n",
    "class decoder_step:\n",
    "    \n",
    "\n",
    "    prev_word = InputLayer((None,),name='decoder prev output inp')\n",
    "\n",
    "    prev_output_emb = EmbeddingLayer(prev_word,\n",
    "                                     n_tokens,\n",
    "                                     word_embedding_size,\n",
    "                                     W=encoder.l_emb.W,\n",
    "                                     name='decoder prev output emb')\n",
    "\n",
    "    #previous GRU state goes here\n",
    "    l_prev_gru0 = InputLayer([None,gru0_units],name='decoder prev gru0')\n",
    "\n",
    "    #gru update\n",
    "    l_gru0 = GRUCell(l_prev_gru0,prev_output_emb,name='decoder gru0',grad_clipping=grad_clip)\n",
    "\n",
    "\n",
    "    #predicted probabilities of next word -- with temperature applied\n",
    "    next_word_probas = HierarchicalSoftmaxDenseLayer(l_gru0,n_tokens)\n",
    "    \n",
    "\n",
    "    #actual next word picked with output probabilities\n",
    "    next_word = ProbabilisticResolver(next_word_probas,\n",
    "                                       assume_normalized=False,\n",
    "                                       name='decoder next letter picker')\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from hsoftmaxplayer import HierarchicalSoftmaxDenseLayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/AgentNet/agentnet/agent/recurrence.py:228: UserWarning: You are giving Recurrence an input sequence of undefined length (None).\n",
      "Make sure it is always above <unspecified>(n_steps) you specified for recurrence\n",
      "  \"Make sure it is always above {}(n_steps) you specified for recurrence\".format(n_steps or \"<unspecified>\"))\n",
      "/root/AgentNet/agentnet/agent/recurrence.py:453: UserWarning: Warning: recurrent loop without unroll_scan got nonempty random state updates list. That happened because there is some source of randomness (e.g. dropout) inside recurrent step graph. To compile such graph, one must either call .get_automatic_updates() right after .get_output and pass these updates to a function, or use no_defalt_updates=True when compiling theano.function.\n",
      "  warn(\"Warning: recurrent loop without unroll_scan got nonempty random state updates list. That happened\"\n",
      "WARNING (theano.tensor.blas): We did not found a dynamic library into the library_dir of the library we use for blas. If you use ATLAS, make sure to compile it with dynamics library.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "###Here we use encoder and decoder_step in the training mode, i.e. feed it with REFERENCE tokens\n",
    "#and ask to predict next reference tokens\n",
    "class training:\n",
    "    \n",
    "    #get previous reference_answers by shifting tensor to the right by 1 tick (e.g. at t=5, prev is t=4)\n",
    "    #and padding with full PAD string as the first input\n",
    "    padding = T.repeat(T.constant(preproc.token_to_ix[\"EOS\"],dtype='int32'),\n",
    "                       reference_answer.shape[0])\n",
    "\n",
    "    prev_reference_answer = T.concatenate([padding[:,None],reference_answer[:,:-1]],axis=1)\n",
    "\n",
    "\n",
    "    l_prev_answers_reference = InputLayer((None,None),prev_reference_answer)\n",
    "\n",
    "    recurrence = Recurrence(state_variables={decoder_step.l_gru0 : decoder_step.l_prev_gru0},\n",
    "                            input_sequences={decoder_step.prev_word : l_prev_answers_reference}, \n",
    "                            state_init = {decoder_step.l_gru0 : encoder.output}, #<- encoder added here\n",
    "                            tracked_outputs=(decoder_step.next_word_probas,decoder_step.next_word), \n",
    "                            unroll_scan = unroll_scan,\n",
    "                            n_steps = max_len if unroll_scan else None,\n",
    "                            )\n",
    "\n",
    "    state_seqs, (probas_seq,output_tokens_seq) = recurrence.get_sequence_layers()\n",
    "\n",
    "\n",
    "    #symbolic output sequences\n",
    "    next_token_probas = lasagne.layers.get_output(probas_seq)\n",
    "\n",
    "    elementwise_ce = lasagne.objectives.categorical_crossentropy(next_token_probas.reshape([-1,n_tokens]),\n",
    "                                                                 reference_answer.ravel()\n",
    "                                                                ).reshape(reference_answer.shape)\n",
    "\n",
    "\n",
    "    #mean crossentropy\n",
    "    loss = (elementwise_ce * reference_mask).sum() / reference_mask.sum()\n",
    "\n",
    "\n",
    "    #all network weights\n",
    "    params = lasagne.layers.get_all_params(recurrence,trainable=True)\n",
    "\n",
    "    #adam one-step weight updates\n",
    "    updates = lasagne.updates.adam(loss,params)\n",
    "\n",
    "\n",
    "    step = theano.function([prev_phrase,reference_answer],loss,\n",
    "                                updates=theano.OrderedUpdates(updates)+recurrence.get_automatic_updates()\n",
    "                                )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### Use the same decoder to generate tokens.\n",
    "# Do this by feeding it's output back as it's input\n",
    "from collections import OrderedDict\n",
    "\n",
    "class generative:\n",
    "\n",
    "\n",
    "    recurrent_states = OrderedDict({\n",
    "            decoder_step.l_gru0:decoder_step.l_prev_gru0,\n",
    "            decoder_step.next_word:decoder_step.prev_word\n",
    "        })\n",
    "    \n",
    "\n",
    "\n",
    "    recurrence = Recurrence(state_variables=recurrent_states,\n",
    "                            state_init = {decoder_step.l_gru0 : encoder.output},\n",
    "                            tracked_outputs=(decoder_step.next_word_probas,decoder_step.next_word),\n",
    "                            batch_size=batch_size,\n",
    "                            n_steps = max_len,\n",
    "                            unroll_scan=unroll_scan,\n",
    "                            )\n",
    "\n",
    "\n",
    "\n",
    "    state_seqs, (probas_seq,output_tokens_seq) = recurrence.get_sequence_layers()\n",
    "\n",
    "    reply_tokens = lasagne.layers.get_output(output_tokens_seq)\n",
    "    \n",
    "    apply_fun = theano.function([prev_phrase],reply_tokens,\n",
    "                                 updates=recurrence.get_automatic_updates())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#If you use separate outputs for context, don't forget about them here\n",
    "from warnings import warn\n",
    "def reply(input_phrase,max_len=None):\n",
    "\n",
    "    input_ix = preproc.phrase_to_ix(input_phrase,max_len=max_len)\n",
    "    \n",
    "    reply_ix = generative.apply_fun([input_ix])[0]\n",
    "\n",
    "    return preproc.ix_to_phrase(reply_ix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "can , hey ! - re !\n"
     ]
    }
   ],
   "source": [
    "#untrained reply dummy (aka interactive mode)\n",
    "print reply(\"What do you think about bidirectional networks?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7285/7285 [00:09<00:00, 730.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuple shapes:\n",
      "(64711, 3, 15) (64711, 15) (64711, 15)\n"
     ]
    }
   ],
   "source": [
    "### Get all tuples (context,input phrase, reference answer)\n",
    "from utils.generate import get_samples_with_context,iterate_minibatches\n",
    "\n",
    "#preprocess and tokenize all phrases;\n",
    "convs_ix = preproc.preprocess_conversations(conversations,verbose=True,max_len=max_len)\n",
    "\n",
    "#get all batches of [phrase context,previous phrase, next phrase]\n",
    "contexts,prev_phrases,reference_answers = get_samples_with_context(convs_ix,\n",
    "                                                                   context_window_size=context_size,\n",
    "                                                                   padder=preproc.token_to_ix[\"PAD\"],\n",
    "                                                                   speaker_filter = lambda s1,s2: True).swapaxes(0,1)\n",
    "#cast to int32\n",
    "contexts,prev_phrases,reference_answers = map(lambda a: np.stack(a).astype('int32'),\n",
    "                                              (contexts,prev_phrases,reference_answers))\n",
    "\n",
    "\n",
    "print \"Tuple shapes:\"\n",
    "print contexts.shape,prev_phrases.shape,reference_answers.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size=32\n",
    "n_epochs=100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "epoch_counter = 1\n",
    "loss_history = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25 iterations...\n",
      "50 iterations...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-72-3e7bc1d1840c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mb_x\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mb_y\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterate_minibatches\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mprev_phrases\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mreference_answers\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m         \u001b[0mloss_history\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb_x\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mb_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0mepoch_counter\u001b[0m \u001b[0;34m+=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python2.7/site-packages/theano/compile/function_module.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    864\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    865\u001b[0m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 866\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0moutput_subset\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    867\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_subset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_subset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    868\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python2.7/site-packages/theano/scan_module/scan_op.pyc\u001b[0m in \u001b[0;36mrval\u001b[0;34m(p, i, o, n, allow_gc)\u001b[0m\n\u001b[1;32m    950\u001b[0m         def rval(p=p, i=node_input_storage, o=node_output_storage, n=node,\n\u001b[1;32m    951\u001b[0m                  allow_gc=allow_gc):\n\u001b[0;32m--> 952\u001b[0;31m             \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    953\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    954\u001b[0m                 \u001b[0mcompute_map\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python2.7/site-packages/theano/scan_module/scan_op.pyc\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(node, args, outs)\u001b[0m\n\u001b[1;32m    939\u001b[0m                         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    940\u001b[0m                         \u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 941\u001b[0;31m                         self, node)\n\u001b[0m\u001b[1;32m    942\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mImportError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtheano\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgof\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMissingGXX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    943\u001b[0m             \u001b[0mp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mtheano/scan_module/scan_perform.pyx\u001b[0m in \u001b[0;36mtheano.scan_module.scan_perform.perform (/root/.theano/compiledir_Linux-4.4-moby-x86_64-with-debian-8.5--2.7.12-64/scan_perform/mod.cpp:4193)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python2.7/site-packages/theano/gof/op.pyc\u001b[0m in \u001b[0;36mrval\u001b[0;34m(p, i, o, n)\u001b[0m\n\u001b[1;32m    863\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mparams\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNoParams\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    864\u001b[0m             \u001b[0;31m# default arguments are stored in the closure of `rval`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 865\u001b[0;31m             \u001b[0;32mdef\u001b[0m \u001b[0mrval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnode_input_storage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnode_output_storage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    866\u001b[0m                 \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    867\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for _ in range(n_epochs):\n",
    "    \n",
    "    for b_x,b_y in iterate_minibatches([prev_phrases,reference_answers],batch_size,shuffle=True):\n",
    "    \n",
    "        loss_history.append(training.step(b_x,b_y))\n",
    "        \n",
    "        epoch_counter +=1\n",
    "        \n",
    "        if epoch_counter %25==0:\n",
    "            print epoch_counter,'iterations...'\n",
    "            \n",
    "        if epoch_counter %100==0:\n",
    "            plt.plot(loss_history);plt.show()\n",
    "            ph =preproc.ix_to_phrase(b_x[0])\n",
    "            print 'A:', ph\n",
    "            print 'B:', reply(ph)\n",
    "            print 'B true:',preproc.ix_to_phrase(b_y[0])\n",
    "\n",
    "    print \"beginning new loop...\"\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
